"""
Meta Traffic DataLoader
æ•´åˆæŸ¥è¯¢é›†å’Œæ”¯æŒé›†çš„æ•°æ®åŠ è½½å™¨ï¼Œä¸MultiMetaFingerNetå®Œå…¨å…¼å®¹
å‚è€ƒFew-shot Detectionçš„è®­ç»ƒæ¨¡å¼
"""

import torch
from torch.utils.data import DataLoader
from typing import Tuple, Dict, List
import sys
import os

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from meta_traffic_dataset import QueryTrafficDataset, SupportTrafficDataset


class MetaTrafficDataLoader:
    """
    Metaå­¦ä¹ æ•°æ®åŠ è½½å™¨
    ç»“åˆæŸ¥è¯¢é›†å’Œæ”¯æŒé›†ï¼Œè¾“å‡ºæ ¼å¼ä¸MultiMetaFingerNet.forward()å®Œå…¨å…¼å®¹
    """
    
    def __init__(self,
                 query_json_path: str,
                 query_files_dir: str,
                 support_root_dir: str,
                 activated_classes: List[int] = None,
                 target_length: int = 30000,
                 shots_per_class: int = 1,
                 batch_size: int = 4,
                 shuffle: bool = True,
                 num_workers: int = 0,
                 random_sampling: bool = False):
        """
        Args:
            query_json_path: æŸ¥è¯¢é›†ç´¢å¼•JSONæ–‡ä»¶è·¯å¾„
            query_files_dir: æŸ¥è¯¢é›†æ•°æ®æ–‡ä»¶ç›®å½•
            support_root_dir: æ”¯æŒé›†æ ¹ç›®å½•
            activated_classes: æ¿€æ´»çš„ç±»åˆ«åˆ—è¡¨ï¼Œé»˜è®¤0-59
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦
            shots_per_class: æ¯ä¸ªç±»åˆ«çš„æ”¯æŒæ ·æœ¬æ•°
            batch_size: æ‰¹å¤§å°
            shuffle: æ˜¯å¦æ‰“ä¹±
            num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°
            random_sampling: æ˜¯å¦ä½¿ç”¨éšæœºé‡‡æ ·æ¨¡å¼ï¼ˆç”¨äºè®­ç»ƒï¼‰
        """
        self.activated_classes = activated_classes if activated_classes else list(range(60))  # 0-59
        self.target_length = target_length
        self.shots_per_class = shots_per_class
        self.batch_size = batch_size
        self.random_sampling = random_sampling
        
        print(f"MetaTrafficDataLoaderåˆå§‹åŒ–...")
        print(f"  - æ¿€æ´»ç±»åˆ«: {len(self.activated_classes)}ä¸ª (0-{max(self.activated_classes)})")
        print(f"  - ç›®æ ‡é•¿åº¦: {target_length}")
        print(f"  - æ¯ç±»æ ·æœ¬æ•°: {shots_per_class}")
        print(f"  - æ‰¹å¤§å°: {batch_size}")
        print(f"  - éšæœºé‡‡æ ·: {random_sampling}")
        
        # åˆå§‹åŒ–æŸ¥è¯¢é›†æ•°æ®é›†
        self.query_dataset = QueryTrafficDataset(
            json_index_path=query_json_path,
            query_files_dir=query_files_dir,
            target_length=target_length,
            activated_classes=self.activated_classes
        )
        
        # åˆå§‹åŒ–æ”¯æŒé›†æ•°æ®é›†
        self.support_dataset = SupportTrafficDataset(
            support_root_dir=support_root_dir,
            activated_classes=self.activated_classes,
            target_length=target_length,
            shots_per_class=shots_per_class,
            random_sampling=random_sampling
        )
        
        # åˆ›å»ºæŸ¥è¯¢é›†DataLoader
        self.query_loader = DataLoader(
            self.query_dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=self._query_collate_fn
        )
        
        if not self.random_sampling:
            # å›ºå®šé‡‡æ ·æ¨¡å¼ï¼šé¢„åŠ è½½æ‰€æœ‰æ”¯æŒé›†æ•°æ®
            self.support_data, self.support_masks, self.class_order = self.support_dataset.get_all_support_data()
            print(f"  - æ”¯æŒé›†å½¢çŠ¶: {self.support_data.shape}")
        else:
            # éšæœºé‡‡æ ·æ¨¡å¼ï¼šæ¯æ¬¡è¿­ä»£æ—¶åŠ¨æ€ç”Ÿæˆæ”¯æŒé›†
            self.class_order = sorted(self.activated_classes)
            print(f"  - æ”¯æŒé›†: åŠ¨æ€éšæœºé‡‡æ ·æ¨¡å¼")
        
        print(f"  - æŸ¥è¯¢é›†æ ·æœ¬æ•°: {len(self.query_dataset)}")
        print(f"  - æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆï¼")
    
    def _query_collate_fn(self, batch):
        """æŸ¥è¯¢é›†çš„collateå‡½æ•°"""
        query_data_list = []
        query_labels_list = []
        metadata_list = []
        
        for query_data, query_labels, metadata in batch:
            query_data_list.append(query_data)
            query_labels_list.append(query_labels)
            metadata_list.append(metadata)
        
        # å †å æˆbatch
        batch_query_data = torch.stack(query_data_list)  # (batch_size, target_length)
        batch_query_labels = torch.stack(query_labels_list)  # (batch_size, num_classes)
        
        return batch_query_data, batch_query_labels, metadata_list
    
    def get_support_data(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–æ”¯æŒé›†æ•°æ®
        
        Returns:
            support_data: (num_classes, shots_per_class, target_length)
            support_masks: (num_classes, shots_per_class, target_length)
        """
        if self.random_sampling:
            # éšæœºé‡‡æ ·æ¨¡å¼ï¼šæ¯æ¬¡è°ƒç”¨éƒ½ç”Ÿæˆæ–°çš„éšæœºæ ·æœ¬
            support_data, support_masks, _ = self.support_dataset.get_all_support_data()
            return support_data, support_masks
        else:
            # å›ºå®šé‡‡æ ·æ¨¡å¼ï¼šè¿”å›é¢„åŠ è½½çš„æ•°æ®
            return self.support_data, self.support_masks
    
    def __iter__(self):
        """è¿”å›æ•°æ®è¿­ä»£å™¨"""
        return MetaTrafficIterator(self)
    
    def __len__(self):
        """è¿”å›batchæ•°é‡"""
        return len(self.query_loader)


class MetaTrafficIterator:
    """
    Meta Trafficæ•°æ®è¿­ä»£å™¨
    è¾“å‡ºæ ¼å¼å®Œå…¨å…¼å®¹MultiMetaFingerNet.forward()
    """
    
    def __init__(self, dataloader: MetaTrafficDataLoader):
        self.dataloader = dataloader
        self.query_iter = iter(dataloader.query_loader)
        self.support_data, self.support_masks = dataloader.get_support_data()
    
    def __iter__(self):
        return self
    
    def __next__(self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Dict]:
        """
        è¿”å›ä¸‹ä¸€ä¸ªbatchï¼Œæ ¼å¼å…¼å®¹MultiMetaFingerNet
        
        Returns:
            query_data: (batch_size, target_length) æŸ¥è¯¢é›†æ•°æ®
            support_data: (num_classes, shots_per_class, target_length) æ”¯æŒé›†æ•°æ®
            support_masks: (num_classes, shots_per_class, target_length) æ”¯æŒé›†mask
            batch_info: Dict åŒ…å«æŸ¥è¯¢æ ‡ç­¾å’Œå…ƒæ•°æ®
        """
        try:
            # è·å–æŸ¥è¯¢é›†batch
            query_data, query_labels, metadata = next(self.query_iter)
            
            # ç»„ç»‡batchä¿¡æ¯
            batch_info = {
                'query_labels': query_labels,  # (batch_size, num_classes)
                'metadata': metadata,
                'class_order': self.dataloader.class_order,
                'num_classes': len(self.dataloader.activated_classes)
            }
            
            return query_data, self.support_data, self.support_masks, batch_info
            
        except StopIteration:
            raise StopIteration


def test_meta_dataloader():
    """æµ‹è¯•æ•´åˆçš„æ•°æ®åŠ è½½å™¨"""
    print("="*60)
    print("æµ‹è¯•MetaTrafficDataLoader")
    print("="*60)
    
    # è®¾ç½®è·¯å¾„
    query_json_path = "/home/ubuntu22/multi-tab-work/meta-finger/data/3tab_task/3tab_train.json"
    query_files_dir = "/home/ubuntu22/multi-tab-work/meta-finger/data/3tab_task/3tab_files"
    support_root_dir = "/home/ubuntu22/multi-tab-work/meta-finger/data/3tab_task/CW_single_tab/train"
    
    # æ£€æŸ¥è·¯å¾„
    paths_to_check = [query_json_path, query_files_dir, support_root_dir]
    for path in paths_to_check:
        if os.path.exists(path):
            print(f"âœ… è·¯å¾„å­˜åœ¨: {path}")
        else:
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
    
    if not all(os.path.exists(p) for p in paths_to_check):
        print("\nâš ï¸  æ•°æ®è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print(f"\nğŸ”„ åˆ›å»ºMetaTrafficDataLoader...")
        dataloader = MetaTrafficDataLoader(
            query_json_path=query_json_path,
            query_files_dir=query_files_dir,
            support_root_dir=support_root_dir,
            activated_classes=list(range(60)),  # 0-59
            target_length=30000,
            shots_per_class=1,
            batch_size=4,
            shuffle=True,
            num_workers=0
        )
        
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®æ ¼å¼å…¼å®¹æ€§...")
        # æµ‹è¯•å‡ ä¸ªbatch
        for i, (query_data, support_data, support_masks, batch_info) in enumerate(dataloader):
            print(f"\nBatch {i+1}:")

            print(f"  æŸ¥è¯¢é›†æ•°æ®: {query_data.shape}")
            print(f"  æŸ¥è¯¢é›†æ ‡ç­¾: {batch_info['query_labels'].shape}")
            print(f"  æ”¯æŒé›†æ•°æ®: {support_data.shape}")
            print(f"  æ”¯æŒé›†æ©ç : {support_masks.shape}")
            print(f"  ç±»åˆ«æ•°é‡: {batch_info['num_classes']}")
            print(f"  æ ‡ç­¾å’Œ: {batch_info['query_labels']}")
            #print(f"  æ”¯æŒé›†æ•°æ®: {support_data[0,0,:100]}")

            # æµ‹è¯•ä¸MultiMetaFingerNetçš„å…¼å®¹æ€§
            print(f"\nğŸ”§ MultiMetaFingerNetå…¼å®¹æ€§æ£€æŸ¥:")
            print(f"  query_dataå½¢çŠ¶: {query_data.shape} â† åº”ä¸º(batch_size, 30000)")
            print(f"  support_dataå½¢çŠ¶: {support_data.shape} â† åº”ä¸º(num_classes, shots, 30000)")
            print(f"  support_maskså½¢çŠ¶: {support_masks.shape} â† åº”ä¸º(num_classes, shots, 30000)")
            
            # åªæµ‹è¯•å‰2ä¸ªbatch
            if i >= 1:
                break
        
        print(f"\nâœ… MetaTrafficDataLoaderæµ‹è¯•å®Œæˆï¼")
        print(f"æ•°æ®æ ¼å¼å®Œå…¨å…¼å®¹MultiMetaFingerNet.forward()æ¥å£")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_meta_dataloader() 