"""
åŸºäºFew-shot Detectionæ€æƒ³çš„ç®€åŒ–æµé‡æ•°æ®åŠ è½½å™¨
å‚è€ƒFewshot_Detection-masterçš„listDatasetå’ŒMetaDatasetè®¾è®¡
é€‚é…Multi-tab Website Fingerprintingåœºæ™¯

å…³é”®è®¾è®¡ï¼š
1. æŸ¥è¯¢é›†ï¼šå¤šæ ‡ç­¾ï¼Œé•¿åº¦30000
2. æ”¯æŒé›†ï¼šæ‰€æœ‰ç±»åˆ«(0-60)ï¼Œé•¿åº¦30000ï¼Œç”¨maskè®°å½•æœ‰æ•ˆéƒ¨åˆ†
3. ç›´æ¥ç±»åˆ«IDæ˜ å°„ï¼Œæ— éœ€å¤æ‚Episodeæ„å»º
"""

import os
import random
import torch
import numpy as np
import pickle
import json
from typing import List, Dict, Tuple, Optional
from torch.utils.data import Dataset


class QueryTrafficDataset(Dataset):
    """
    æŸ¥è¯¢é›†æ•°æ®åŠ è½½å™¨
    å‚è€ƒFew-shot Detectionçš„listDatasetè®¾è®¡
    """
    
    def __init__(self, 
                 json_index_path: str,
                 query_files_dir: str,
                 target_length: int = 30000,
                 activated_classes: List[int] = None):
        """
        Args:
            json_index_path: æŸ¥è¯¢é›†ç´¢å¼•JSONæ–‡ä»¶è·¯å¾„
            query_files_dir: æŸ¥è¯¢é›†æ•°æ®æ–‡ä»¶ç›®å½•  
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦
            activated_classes: æ¿€æ´»çš„ç±»åˆ«åˆ—è¡¨ï¼Œé»˜è®¤0-59
        """
        self.json_index_path = json_index_path
        self.query_files_dir = query_files_dir
        self.target_length = target_length
        self.activated_classes = activated_classes if activated_classes else list(range(60))  # 0-59
        
        # åŠ è½½æŸ¥è¯¢é›†ç´¢å¼•
        self._load_query_index()
        
        print(f"QueryTrafficDatasetåˆå§‹åŒ–å®Œæˆ:")
        print(f"  - æŸ¥è¯¢æ ·æœ¬æ•°é‡: {len(self.query_index)}")
        print(f"  - æ¿€æ´»ç±»åˆ«æ•°é‡: {len(self.activated_classes)}")
        print(f"  - ç›®æ ‡åºåˆ—é•¿åº¦: {self.target_length}")
    
    def _load_query_index(self):
        """åŠ è½½æŸ¥è¯¢é›†ç´¢å¼•"""
        if not os.path.exists(self.json_index_path):
            raise FileNotFoundError(f"JSONç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.json_index_path}")
        
        with open(self.json_index_path, 'r') as f:
            query_file_names = json.load(f)
        
        self.query_index = []
        for filename in query_file_names:
            # è§£ææ–‡ä»¶åä¸­çš„æ ‡ç­¾
            labels = self._parse_labels_from_filename(filename)
            if labels:  # åªä¿ç•™æœ‰æ•ˆæ ‡ç­¾çš„æ–‡ä»¶
                file_path = os.path.join(self.query_files_dir, filename)
                self.query_index.append({
                    'filename': filename,
                    'labels': labels,
                    'file_path': file_path
                })
        
        print(f"æœ‰æ•ˆæŸ¥è¯¢æ ·æœ¬æ•°é‡: {len(self.query_index)}")
    
    def _parse_labels_from_filename(self, filename: str) -> List[int]:
        """
        ä»æ–‡ä»¶åè§£ææ ‡ç­¾
        æ–‡ä»¶åæ ¼å¼: "ç±»åˆ«1_ç±»åˆ«2_ç±»åˆ«3_éšæœºæ–‡ä»¶å.pkl"
        """
        basename = os.path.splitext(filename)[0]
        parts = basename.split('_')
        
        labels = []
        for part in parts:
            try:
                label = int(part)
                if label in self.activated_classes:
                    labels.append(label)
            except ValueError:
                # éæ•°å­—éƒ¨åˆ†è®¤ä¸ºæ˜¯éšæœºæ–‡ä»¶åï¼Œåœæ­¢è§£æ
                break
        
        return labels
    
    def _process_sequence(self, raw_data: List) -> torch.Tensor:
        """
        å¤„ç†åºåˆ—æ•°æ®ï¼šæˆªæ–­æˆ–å¡«å……åˆ°ç›®æ ‡é•¿åº¦
        """
        if len(raw_data) >= self.target_length:
            # æˆªæ–­
            processed = raw_data[:self.target_length]
        else:
            # å¡«å……0
            processed = raw_data + [0] * (self.target_length - len(raw_data))
        
        return torch.tensor(processed, dtype=torch.float32)
    
    def _labels_to_multihot(self, labels: List[int]) -> torch.Tensor:
        """
        å°†æ ‡ç­¾åˆ—è¡¨è½¬æ¢ä¸ºå¤šçƒ­ç¼–ç 
        """
        num_classes = len(self.activated_classes)
        multihot = torch.zeros(num_classes, dtype=torch.float32)
        
        for label in labels:
            if label in self.activated_classes:
                idx = self.activated_classes.index(label)
                multihot[idx] = 1.0
                
        return multihot
    
    def __len__(self) -> int:
        return len(self.query_index)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        """
        è·å–æŸ¥è¯¢æ ·æœ¬
        
        Returns:
            query_data: (target_length,) æŸ¥è¯¢åºåˆ—
            query_labels: (num_classes,) å¤šçƒ­ç¼–ç æ ‡ç­¾
            metadata: å…ƒæ•°æ®å­—å…¸
        """
        sample_info = self.query_index[idx]
        
        # åŠ è½½æ•°æ®
        with open(sample_info['file_path'], 'rb') as f:
            sample_data = pickle.load(f)
        
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(sample_data, dict) and 'data' in sample_data:
            raw_data = sample_data['data']
        elif isinstance(sample_data, (list, np.ndarray)):
            raw_data = sample_data
        else:
            # å…¶ä»–æ ¼å¼ç›´æ¥ä½¿ç”¨
            raw_data = sample_data
        
        # ç¡®ä¿raw_dataæ˜¯åˆ—è¡¨æ ¼å¼
        if isinstance(raw_data, np.ndarray):
            raw_data = raw_data.tolist()
        elif not isinstance(raw_data, list):
            raw_data = [raw_data]
        
        # å¤„ç†åºåˆ—
        query_data = self._process_sequence(raw_data)
        
        # å¤„ç†æ ‡ç­¾
        query_labels = self._labels_to_multihot(sample_info['labels']) ##Note å¦‚æœéœ€è¦é¡ºåºä¿¡æ¯ï¼Œåˆ™ä¸å¯ä½¿ç”¨Multihot
        
        # å…ƒæ•°æ®
        metadata = {
            'filename': sample_info['filename'],
            'original_labels': sample_info['labels'],
            'file_path': sample_info['file_path']
        }
        
        return query_data, query_labels, metadata


class SupportTrafficDataset(Dataset):
    """
    æ”¯æŒé›†æ•°æ®åŠ è½½å™¨
    å‚è€ƒFew-shot Detectionçš„MetaDatasetè®¾è®¡
    ä¸ºæ‰€æœ‰ç±»åˆ«ç”Ÿæˆæ”¯æŒé›†ï¼Œæ— éœ€Episodeé‡‡æ ·
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šå›ºå®šé‡‡æ ·ï¼ˆç”¨äºfew-shotè°ƒæ•´ï¼‰å’Œéšæœºé‡‡æ ·ï¼ˆç”¨äºè®­ç»ƒï¼‰
    """
    
    def __init__(self,
                 support_root_dir: str,
                 activated_classes: List[int] = None,
                 target_length: int = 30000,
                 shots_per_class: int = 1,
                 random_sampling: bool = False):
        """
        Args:
            support_root_dir: æ”¯æŒé›†æ ¹ç›®å½•
            activated_classes: æ¿€æ´»çš„ç±»åˆ«åˆ—è¡¨ï¼Œé»˜è®¤0-59
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦ï¼ˆä¿®æ­£ä¸º30000ï¼‰
            shots_per_class: æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
            random_sampling: æ˜¯å¦ä½¿ç”¨éšæœºé‡‡æ ·æ¨¡å¼ï¼ˆTrueï¼šæ¯æ¬¡éšæœºé€‰æ‹©ï¼ŒFalseï¼šå›ºå®šé€‰æ‹©ï¼‰
        """
        self.support_root_dir = support_root_dir
        self.activated_classes = activated_classes if activated_classes else list(range(60))  # 0-59
        self.target_length = target_length
        self.shots_per_class = shots_per_class
        self.random_sampling = random_sampling
        
        # æ„å»ºæ”¯æŒé›†ç´¢å¼•
        self._build_support_index()
        
        if not self.random_sampling:
            # å›ºå®šé‡‡æ ·æ¨¡å¼ï¼šé¢„ç”Ÿæˆæ‰€æœ‰ç±»åˆ«çš„æ”¯æŒé›†
            self._prepare_all_support_data()
        else:
            # éšæœºé‡‡æ ·æ¨¡å¼ï¼šä»…è®°å½•æ–‡ä»¶ç´¢å¼•ï¼Œæ¯æ¬¡åŠ¨æ€åŠ è½½
            print(f"SupportTrafficDatasetåˆå§‹åŒ–å®Œæˆ (éšæœºé‡‡æ ·æ¨¡å¼):")
            print(f"  - æ¿€æ´»ç±»åˆ«æ•°é‡: {len(self.activated_classes)}")
            print(f"  - æ¯ç±»æ ·æœ¬æ•°: {self.shots_per_class}")
            print(f"  - ç›®æ ‡åºåˆ—é•¿åº¦: {self.target_length}")
            print(f"  - éšæœºé‡‡æ ·: {self.random_sampling}")
    
    def _build_support_index(self):
        """æ„å»ºæ”¯æŒé›†ç´¢å¼•"""
        self.support_files_by_class = {}
        
        for class_id in self.activated_classes:
            class_dir = os.path.join(self.support_root_dir, str(class_id))
            if not os.path.exists(class_dir):
                print(f"è­¦å‘Š: ç±»åˆ«{class_id}çš„ç›®å½•ä¸å­˜åœ¨: {class_dir}")
                continue
            
            # æ”¶é›†è¯¥ç±»åˆ«çš„æ‰€æœ‰pklæ–‡ä»¶
            class_files = [
                os.path.join(class_dir, f) 
                for f in os.listdir(class_dir) 
                if f.endswith('.pkl')
            ]
            
            if len(class_files) < self.shots_per_class:
                print(f"è­¦å‘Š: ç±»åˆ«{class_id}æ ·æœ¬ä¸è¶³ï¼Œéœ€è¦{self.shots_per_class}ä¸ªï¼Œåªæœ‰{len(class_files)}ä¸ª")
            
            self.support_files_by_class[class_id] = class_files
            print(f"ç±»åˆ«{class_id}: æ‰¾åˆ°{len(class_files)}ä¸ªæ”¯æŒæ ·æœ¬")
    
    def _process_support_sequence(self, raw_data: List) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¤„ç†æ”¯æŒé›†åºåˆ—ï¼šè¡¥é½åˆ°ç›®æ ‡é•¿åº¦å¹¶ç”Ÿæˆmask
        
        Returns:
            data: (target_length,) è¡¥é½åçš„åºåˆ—
            mask: (target_length,) æœ‰æ•ˆæ•°æ®maskï¼Œ1è¡¨ç¤ºæœ‰æ•ˆï¼Œ0è¡¨ç¤ºå¡«å……
        """
        original_length = len(raw_data)
        
        if original_length >= self.target_length:
            # æˆªæ–­
            data = raw_data[:self.target_length]
            mask = torch.ones(self.target_length, dtype=torch.bool)
        else:
            # å¡«å……0
            data = raw_data + [0] * (self.target_length - original_length)
            mask = torch.zeros(self.target_length, dtype=torch.bool)
            mask[:original_length] = True
        
        return torch.tensor(data, dtype=torch.float32), mask
    
    def _prepare_all_support_data(self):
        """é¢„ç”Ÿæˆæ‰€æœ‰ç±»åˆ«çš„æ”¯æŒé›†æ•°æ®ï¼ˆå›ºå®šé‡‡æ ·æ¨¡å¼ï¼‰"""
        self.all_support_data = []
        self.all_support_masks = []
        self.class_order = []  # è®°å½•ç±»åˆ«é¡ºåºï¼Œç¡®ä¿ç´¢å¼•å¯¹åº”
        
        for class_id in sorted(self.activated_classes):  # æ’åºç¡®ä¿ä¸€è‡´æ€§
            if class_id not in self.support_files_by_class:
                # å¦‚æœæŸä¸ªç±»åˆ«æ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºé›¶å‘é‡
                print(f"è­¦å‘Š: ç±»åˆ«{class_id}æ²¡æœ‰æ”¯æŒæ ·æœ¬")
                exit(0)
            
            class_files = self.support_files_by_class[class_id]
            
            for shot_idx in range(self.shots_per_class):
                # å›ºå®šé€‰æ‹©æ–‡ä»¶ï¼ˆå¾ªç¯é€‰æ‹©ï¼‰
                if len(class_files) > 0:
                    file_idx = shot_idx % len(class_files)
                    file_path = class_files[file_idx]
                    
                    # åŠ è½½æ•°æ®
                    data, mask = self._load_and_process_sample(file_path)
                else:
                    data = torch.zeros(self.target_length, dtype=torch.float32)
                    mask = torch.zeros(self.target_length, dtype=torch.bool)
                
                self.all_support_data.append(data)
                self.all_support_masks.append(mask)
                self.class_order.append(class_id)
        
        # è½¬æ¢ä¸ºtensor
        # shape: (num_classes * shots_per_class, target_length)
        self.support_data_tensor = torch.stack(self.all_support_data)
        self.support_masks_tensor = torch.stack(self.all_support_masks)
        
        print(f"æ”¯æŒé›†æ•°æ®å‡†å¤‡å®Œæˆ (å›ºå®šé‡‡æ ·æ¨¡å¼):")
        print(f"  - æ”¯æŒé›†å½¢çŠ¶: {self.support_data_tensor.shape}")
        print(f"  - æ©ç å½¢çŠ¶: {self.support_masks_tensor.shape}")
        print(f"  - ç±»åˆ«é¡ºåº: {self.class_order}")
    
    def _load_and_process_sample(self, file_path: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åŠ è½½å¹¶å¤„ç†å•ä¸ªæ ·æœ¬æ–‡ä»¶
        
        Args:
            file_path: æ ·æœ¬æ–‡ä»¶è·¯å¾„
            
        Returns:
            data: (target_length,) å¤„ç†åçš„åºåˆ—
            mask: (target_length,) æœ‰æ•ˆæ•°æ®mask
        """
        try:
            with open(file_path, 'rb') as f:
                sample_data = pickle.load(f)
            
            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
            if isinstance(sample_data, dict) and 'data' in sample_data:
                raw_data = sample_data['data']
            elif isinstance(sample_data, (list, np.ndarray)):
                raw_data = sample_data
            else:
                # å…¶ä»–æ ¼å¼ç›´æ¥ä½¿ç”¨
                raw_data = sample_data
            
            # ç¡®ä¿raw_dataæ˜¯åˆ—è¡¨æ ¼å¼
            if isinstance(raw_data, np.ndarray):
                raw_data = raw_data.tolist()
            elif not isinstance(raw_data, list):
                raw_data = [raw_data]
            
            # å¤„ç†åºåˆ—å’Œmask
            data, mask = self._process_support_sequence(raw_data)
            return data, mask
            
        except Exception as e:
            print(f"è­¦å‘Š: åŠ è½½æ–‡ä»¶{file_path}å¤±è´¥: {e}ï¼Œä½¿ç”¨é›¶å‘é‡")
            data = torch.zeros(self.target_length, dtype=torch.float32)
            mask = torch.zeros(self.target_length, dtype=torch.bool)
            return data, mask
    
    def _generate_random_support_batch(self) -> Tuple[torch.Tensor, torch.Tensor, List[int]]:
        """
        éšæœºç”Ÿæˆä¸€ä¸ªæ”¯æŒé›†batchï¼ˆéšæœºé‡‡æ ·æ¨¡å¼ï¼‰
        
        Returns:
            support_data: (num_classes, shots_per_class, target_length)
            support_masks: (num_classes, shots_per_class, target_length)
            class_order: List[int] ç±»åˆ«é¡ºåºï¼ˆä¿æŒ0-59é¡ºåºï¼‰
        """
        import random
        
        batch_support_data = []
        batch_support_masks = []
        class_order = sorted(self.activated_classes)  # ä¿æŒé¡ºåºä¸€è‡´æ€§
        
        for class_id in class_order:
            if class_id not in self.support_files_by_class:
                print(f"è­¦å‘Š: ç±»åˆ«{class_id}æ²¡æœ‰æ”¯æŒæ ·æœ¬")
                exit(0)
            
            class_files = self.support_files_by_class[class_id]
            
            for shot_idx in range(self.shots_per_class):
                if len(class_files) > 0:
                    # éšæœºé€‰æ‹©æ–‡ä»¶
                    file_path = random.choice(class_files)
                    data, mask = self._load_and_process_sample(file_path)
                
                batch_support_data.append(data)
                batch_support_masks.append(mask)
        
        # è½¬æ¢ä¸ºtensorå¹¶é‡æ•´å½¢
        num_classes = len(class_order)
        support_data_tensor = torch.stack(batch_support_data)
        support_masks_tensor = torch.stack(batch_support_masks)
        
        # é‡æ•´å½¢ä¸º (num_classes, shots_per_class, target_length)
        support_data = support_data_tensor.view(
            num_classes, self.shots_per_class, self.target_length
        )
        support_masks = support_masks_tensor.view(
            num_classes, self.shots_per_class, self.target_length
        )
        
        return support_data, support_masks, class_order

    def get_all_support_data(self) -> Tuple[torch.Tensor, torch.Tensor, List[int]]:
        """
        è·å–æ‰€æœ‰ç±»åˆ«çš„æ”¯æŒé›†æ•°æ®
        
        Returns:
            support_data: (num_classes, shots_per_class, target_length)
            support_masks: (num_classes, shots_per_class, target_length)  
            class_order: List[int] ç±»åˆ«é¡ºåº
        """
        if self.random_sampling:
            # éšæœºé‡‡æ ·æ¨¡å¼ï¼šæ¯æ¬¡è°ƒç”¨éƒ½ç”Ÿæˆæ–°çš„éšæœºæ ·æœ¬
            return self._generate_random_support_batch()
        else:
            # å›ºå®šé‡‡æ ·æ¨¡å¼ï¼šè¿”å›é¢„ç”Ÿæˆçš„æ•°æ®
            num_classes = len(self.activated_classes)
            
            # é‡æ•´å½¢ä¸º (num_classes, shots_per_class, target_length)
            support_data = self.support_data_tensor.view(
                num_classes, self.shots_per_class, self.target_length
            )
            support_masks = self.support_masks_tensor.view(
                num_classes, self.shots_per_class, self.target_length
            )
            
            return support_data, support_masks, self.activated_classes
    
    def __len__(self) -> int:
        return len(self.all_support_data)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:
        """
        è·å–å•ä¸ªæ”¯æŒæ ·æœ¬ï¼ˆé€šå¸¸ä¸ç›´æ¥ä½¿ç”¨ï¼Œä¸»è¦ç”¨get_all_support_dataï¼‰
        """
        return (
            self.all_support_data[idx],
            self.all_support_masks[idx], 
            self.class_order[idx]
        )


def test_datasets():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print("="*60)
    print("æµ‹è¯•Meta Traffic Dataset")
    print("="*60)
    
    # è®¾ç½®è·¯å¾„ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
    query_json_path = "/home/ubuntu22/multi-tab-work/meta-finger/data/3tab_task/3tab_train.json"
    query_files_dir = "/home/ubuntu22/multi-tab-work/meta-finger/data/3tab_task/3tab_files"
    support_root_dir = "/home/ubuntu22/multi-tab-work/meta-finger/data/3tab_task/CW_single_tab/train"
    
    # æ£€æŸ¥è·¯å¾„å­˜åœ¨æ€§
    paths_to_check = [query_json_path, query_files_dir, support_root_dir]
    for path in paths_to_check:
        if os.path.exists(path):
            print(f"âœ… è·¯å¾„å­˜åœ¨: {path}")
        else:
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
    
    # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæµ‹è¯•
    if not all(os.path.exists(p) for p in paths_to_check):
        print("\nâš ï¸  å®é™…æ•°æ®è·¯å¾„ä¸å­˜åœ¨ï¼Œè·³è¿‡æ•°æ®åŠ è½½æµ‹è¯•")
        print("è¯·ç¡®ä¿ä»¥ä¸‹è·¯å¾„å­˜åœ¨åå†æµ‹è¯•:")
        for path in paths_to_check:
            print(f"  - {path}")
        return
    
    try:
        # æµ‹è¯•æŸ¥è¯¢é›†
        print("\nğŸ“Š æµ‹è¯•æŸ¥è¯¢é›†æ•°æ®åŠ è½½å™¨...")
        query_dataset = QueryTrafficDataset(
            json_index_path=query_json_path,
            query_files_dir=query_files_dir,
            target_length=30000,
            activated_classes=list(range(60))  # 0-59
        )
        
        if len(query_dataset) > 0:
            query_data, query_labels, metadata = query_dataset[0]
            print(f"æŸ¥è¯¢æ ·æœ¬æµ‹è¯•:")
            print(f"  - æ•°æ®å½¢çŠ¶: {query_data.shape}")
            print(f"  - æ ‡ç­¾å½¢çŠ¶: {query_labels.shape}")
            print(f"  - æ ‡ç­¾å’Œ: {query_labels.sum().item()}")
            print(f"  - æ–‡ä»¶å: {metadata['filename']}")
        
        # æµ‹è¯•æ”¯æŒé›†
        print("\nğŸ¯ æµ‹è¯•æ”¯æŒé›†æ•°æ®åŠ è½½å™¨...")
        support_dataset = SupportTrafficDataset(
            support_root_dir=support_root_dir,
            activated_classes=list(range(60)),  # 0-59
            target_length=30000,
            shots_per_class=1
        )
        
        support_data, support_masks, class_order = support_dataset.get_all_support_data()
        print(f"æ”¯æŒé›†æµ‹è¯•:")
        print(f"  - æ”¯æŒé›†æ•°æ®å½¢çŠ¶: {support_data.shape}")
        print(f"  - æ”¯æŒé›†æ©ç å½¢çŠ¶: {support_masks.shape}")
        print(f"  - ç±»åˆ«é¡ºåº: {class_order[:10]}...ï¼ˆæ˜¾ç¤ºå‰10ä¸ªï¼‰")
        
        # æµ‹è¯•å…¼å®¹æ€§
        print(f"\nğŸ”„ å…¼å®¹æ€§æµ‹è¯•:")
        print(f"  - æŸ¥è¯¢é›†é•¿åº¦: {query_data.shape[0]}")
        print(f"  - æ”¯æŒé›†é•¿åº¦: {support_data.shape[2]}")
        print(f"  - é•¿åº¦åŒ¹é…: {'âœ…' if query_data.shape[0] == support_data.shape[2] else 'âŒ'}")
        
        print(f"\nâœ… æ•°æ®åŠ è½½å™¨æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_datasets() 