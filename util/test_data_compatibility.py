"""
æµ‹è¯•ç”Ÿæˆçš„3æ ‡ç­¾æ•°æ®ä¸MetaTrafficDataLoaderçš„å…¼å®¹æ€§
"""

import os
import sys
import json
import pickle
import numpy as np
import random

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_generated_data_format():
    """æµ‹è¯•ç”Ÿæˆæ•°æ®çš„æ ¼å¼æ­£ç¡®æ€§"""
    print("ğŸ” æµ‹è¯•ç”Ÿæˆæ•°æ®æ ¼å¼...")
    
    query_data_dir = "datasets/3tab_exp/base_train/query_data"
    
    # éšæœºé€‰æ‹©å‡ ä¸ªæ–‡ä»¶è¿›è¡Œæµ‹è¯•
    pkl_files = [f for f in os.listdir(query_data_dir) if f.endswith('.pkl')]
    test_files = random.sample(pkl_files, min(5, len(pkl_files)))
    
    for filename in test_files:
        file_path = os.path.join(query_data_dir, filename)
        print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶: {filename}")
        
        try:
            with open(file_path, 'rb') as f:
                sample = pickle.load(f)
            
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = ['time', 'data', 'labels', 'source_files']
            for field in required_fields:
                if field not in sample:
                    print(f"  âŒ ç¼ºå°‘å­—æ®µ: {field}")
                    continue
                else:
                    print(f"  âœ… å­—æ®µ {field}: å­˜åœ¨")
            
            # æ£€æŸ¥æ•°æ®æ ¼å¼
            time_data = sample['time']
            data_sequence = sample['data']
            labels = sample['labels']
            
            print(f"  ğŸ“Š æ—¶é—´åºåˆ—é•¿åº¦: {len(time_data)}")
            print(f"  ğŸ“Š æ•°æ®åºåˆ—é•¿åº¦: {len(data_sequence)}")
            print(f"  ğŸ“Š æ ‡ç­¾: {labels}")
            print(f"  ğŸ“Š æ•°æ®èŒƒå›´: [{np.min(data_sequence):.1f}, {np.max(data_sequence):.1f}]")
            
            # éªŒè¯æ ‡ç­¾é¡ºåºä¸æ–‡ä»¶åçš„å¯¹åº”
            filename_labels = filename.split('_')[:3]
            filename_labels = [int(x) for x in filename_labels]
            
            if filename_labels == labels:
                print(f"  âœ… æ ‡ç­¾é¡ºåºä¸æ–‡ä»¶åä¸€è‡´: {labels}")
            else:
                print(f"  âŒ æ ‡ç­¾é¡ºåºä¸ä¸€è‡´: æ–‡ä»¶å{filename_labels} vs æ•°æ®{labels}")
            
            print(f"  âœ… æ ¼å¼éªŒè¯é€šè¿‡")
            
        except Exception as e:
            print(f"  âŒ è¯»å–å¤±è´¥: {e}")

def test_json_index_format():
    """æµ‹è¯•JSONç´¢å¼•æ–‡ä»¶æ ¼å¼"""
    print(f"\nğŸ” æµ‹è¯•JSONç´¢å¼•æ ¼å¼...")
    
    json_files = [
        "datasets/3tab_exp/base_train/3tab_train.json",
        "datasets/3tab_exp/base_train/3tab_val.json"
    ]
    
    for json_file in json_files:
        print(f"\nğŸ“ æµ‹è¯•æ–‡ä»¶: {json_file}")
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"  ğŸ“Š ç´¢å¼•æ¡ç›®æ•°é‡: {len(data)}")
            
            # æ£€æŸ¥å‰å‡ ä¸ªæ¡ç›®çš„æ ¼å¼
            for i, entry in enumerate(data[:3]):
                print(f"  ğŸ“ æ¡ç›®{i+1}:")
                print(f"    - filename: {entry.get('filename', 'N/A')}")
                print(f"    - labels: {entry.get('labels', 'N/A')}")
                print(f"    - data_length: {entry.get('data_length', 'N/A')}")
                print(f"    - time_range: {entry.get('time_range', 'N/A')}")
            
            print(f"  âœ… JSONæ ¼å¼éªŒè¯é€šè¿‡")
            
        except Exception as e:
            print(f"  âŒ JSONè¯»å–å¤±è´¥: {e}")

def test_dataloader_compatibility():
    """æµ‹è¯•ä¸MetaTrafficDataLoaderçš„å…¼å®¹æ€§"""
    print(f"\nğŸ” æµ‹è¯•ä¸MetaTrafficDataLoaderçš„å…¼å®¹æ€§...")
    
    try:
        # å¯¼å…¥æ•°æ®åŠ è½½å™¨
        from data.meta_traffic_dataloader import MetaTrafficDataLoader
        
        # é…ç½®å‚æ•°ï¼ˆä½¿ç”¨æ–°ç”Ÿæˆçš„æ•°æ®ï¼‰
        query_json_path = "datasets/3tab_exp/base_train/3tab_train.json"
        query_files_dir = "datasets/3tab_exp/base_train/query_data"
        support_root_dir = "datasets/3tab_exp/base_train/support_data"
        
        print(f"  ğŸ“‹ é…ç½®å‚æ•°:")
        print(f"    - query_json: {query_json_path}")
        print(f"    - query_dir: {query_files_dir}")
        print(f"    - support_dir: {support_root_dir}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = MetaTrafficDataLoader(
            query_json_path=query_json_path,
            query_files_dir=query_files_dir,
            support_root_dir=support_root_dir,
            activated_classes=list(range(60)),  # 0-59ç±»åˆ«
            target_length=30000,
            shots_per_class=1,
            batch_size=2,  # å°æ‰¹é‡æµ‹è¯•
            shuffle=True,
            num_workers=0,
            random_sampling=True  # ä½¿ç”¨éšæœºé‡‡æ ·æµ‹è¯•
        )
        
        print(f"  âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"  ğŸ“Š æ•°æ®åŠ è½½å™¨é•¿åº¦: {len(dataloader)}")
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        print(f"  ğŸ”„ æµ‹è¯•æ•°æ®åŠ è½½...")
        
        for i, batch in enumerate(dataloader):
            query_data, support_data, support_masks, batch_info = batch
            
            print(f"  ğŸ“¦ Batch {i+1}:")
            print(f"    - query_data shape: {query_data.shape}")
            print(f"    - support_data shape: {support_data.shape}")
            print(f"    - support_masks shape: {support_masks.shape}")
            print(f"    - query_labels shape: {batch_info['query_labels'].shape}")
            
            # æ£€æŸ¥æ•°æ®å†…å®¹
            query_labels = batch_info['query_labels']
            print(f"    - æŸ¥è¯¢æ ‡ç­¾ç¤ºä¾‹: {query_labels[0].nonzero().flatten().tolist()}")
            
            # åªæµ‹è¯•å‰3ä¸ªbatch
            if i >= 2:
                break
        
        print(f"  âœ… æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡")
        
        # æµ‹è¯•å›ºå®šé‡‡æ ·æ¨¡å¼
        print(f"  ğŸ”„ æµ‹è¯•å›ºå®šé‡‡æ ·æ¨¡å¼...")
        
        val_dataloader = MetaTrafficDataLoader(
            query_json_path="datasets/3tab_exp/base_train/3tab_val.json",
            query_files_dir=query_files_dir,
            support_root_dir=support_root_dir,
            activated_classes=list(range(60)),
            target_length=30000,
            shots_per_class=1,
            batch_size=2,
            shuffle=False,
            num_workers=0,
            random_sampling=False  # ä½¿ç”¨å›ºå®šé‡‡æ ·æµ‹è¯•
        )
        
        for i, batch in enumerate(val_dataloader):
            query_data, support_data, support_masks, batch_info = batch
            print(f"  ğŸ“¦ éªŒè¯é›† Batch {i+1}: query_data {query_data.shape}")
            if i >= 1:
                break
        
        print(f"  âœ… å›ºå®šé‡‡æ ·æ¨¡å¼æµ‹è¯•é€šè¿‡")
        
    except ImportError as e:
        print(f"  âŒ å¯¼å…¥MetaTrafficDataLoaderå¤±è´¥: {e}")
    except Exception as e:
        print(f"  âŒ å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")

def test_model_integration():
    """æµ‹è¯•ä¸æ¨¡å‹çš„é›†æˆ"""
    print(f"\nğŸ” æµ‹è¯•ä¸MultiMetaFingerNetçš„é›†æˆ...")
    
    try:
        import torch
        from models.feature_extractors import MultiMetaFingerNet
        from data.meta_traffic_dataloader import MetaTrafficDataLoader
        
        # åˆ›å»ºæ¨¡å‹
        model = MultiMetaFingerNet(
            num_classes=60,
            dropout=0.5,
            support_blocks=3
        )
        
        print(f"  âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = MetaTrafficDataLoader(
            query_json_path="datasets/3tab_exp/base_train/3tab_train.json",
            query_files_dir="datasets/3tab_exp/base_train/query_data",
            support_root_dir="datasets/3tab_exp/base_train/support_data",
            activated_classes=list(range(60)),
            target_length=30000,
            shots_per_class=1,
            batch_size=1,  # å•æ ·æœ¬æµ‹è¯•
            shuffle=False,
            num_workers=0,
            random_sampling=True
        )
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        print(f"  ğŸ”„ æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
        
        for i, batch in enumerate(dataloader):
            query_data, support_data, support_masks, batch_info = batch
            
            # æ¨¡å‹å‰å‘ä¼ æ’­
            with torch.no_grad():
                results = model(query_data, support_data, support_masks)
            
            print(f"  ğŸ“¦ å‰å‘ä¼ æ’­ç»“æœ:")
            print(f"    - logits shape: {results['logits'].shape}")
            print(f"    - reweighted_features shape: {results['reweighted_features'].shape}")
            
            # åªæµ‹è¯•ä¸€ä¸ªbatch
            break
        
        print(f"  âœ… æ¨¡å‹é›†æˆæµ‹è¯•é€šè¿‡")
        
    except ImportError as e:
        print(f"  âŒ å¯¼å…¥æ¨¡å‹å¤±è´¥: {e}")
    except Exception as e:
        print(f"  âŒ æ¨¡å‹é›†æˆæµ‹è¯•å¤±è´¥: {e}")

def generate_final_report():
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    print(f"\nğŸ“Š æœ€ç»ˆæ•°æ®ç»Ÿè®¡æŠ¥å‘Š:")
    
    # æ”¯æŒé›†ç»Ÿè®¡
    support_data_dir = "datasets/3tab_exp/base_train/support_data"
    support_classes = len([d for d in os.listdir(support_data_dir) if os.path.isdir(os.path.join(support_data_dir, d))])
    
    # æŸ¥è¯¢é›†ç»Ÿè®¡
    query_data_dir = "datasets/3tab_exp/base_train/query_data"
    query_files = len([f for f in os.listdir(query_data_dir) if f.endswith('.pkl')])
    
    # JSONç´¢å¼•ç»Ÿè®¡
    with open("datasets/3tab_exp/base_train/3tab_train.json", 'r') as f:
        train_data = json.load(f)
    
    with open("datasets/3tab_exp/base_train/3tab_val.json", 'r') as f:
        val_data = json.load(f)
    
    print(f"  ğŸ“ æ”¯æŒé›†æ•°æ®:")
    print(f"    - ç±»åˆ«æ•°é‡: {support_classes}")
    print(f"    - æ€»æ ·æœ¬æ•°: {support_classes * 50} (æ¯ç±»50ä¸ª)")
    
    print(f"  ğŸ“ æŸ¥è¯¢é›†æ•°æ®:")
    print(f"    - æ€»æ–‡ä»¶æ•°: {query_files}")
    print(f"    - è®­ç»ƒé›†ç´¢å¼•: {len(train_data)}æ¡")
    print(f"    - éªŒè¯é›†ç´¢å¼•: {len(val_data)}æ¡")
    
    print(f"  ğŸ¯ æ•°æ®è¦†ç›–:")
    print(f"    - 3ç±»åˆ«ç»„åˆæ•°: C(60,3) = 34,220ç§")
    print(f"    - æ¯ç»„åˆæ ·æœ¬æ•°: 5ä¸ª")
    print(f"    - ç†è®ºæ€»æ ·æœ¬æ•°: 171,100ä¸ª")
    print(f"    - å®é™…ç”Ÿæˆæ ·æœ¬æ•°: {len(train_data) + len(val_data)}ä¸ª")
    print(f"    - ç”ŸæˆæˆåŠŸç‡: {(len(train_data) + len(val_data))/171100*100:.2f}%")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ 3æ ‡ç­¾æ•°æ®å…¼å®¹æ€§æµ‹è¯•")
    print("="*50)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)
    
    # ä¾æ¬¡æ‰§è¡Œå„é¡¹æµ‹è¯•
    test_generated_data_format()
    #test_json_index_format()
    test_dataloader_compatibility()
    test_model_integration()
    generate_final_report()
    
    print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print(f"âœ… æ•°æ®ç”Ÿæˆå’Œå…¼å®¹æ€§éªŒè¯å…¨éƒ¨é€šè¿‡")
    print(f"ğŸš€ å¯ä»¥å¼€å§‹ä½¿ç”¨æ–°æ•°æ®è¿›è¡Œè®­ç»ƒ")

if __name__ == "__main__":
    main() 