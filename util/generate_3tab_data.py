"""
3æ ‡ç­¾å¤šæ ‡ç­¾æ•°æ®ç”Ÿæˆè„šæœ¬
åŸºäºprocess_ARES_data.pyä¸­çš„merge_with_durationscaleå‡½æ•°
å®ç°æµå¼å¤„ç†å’Œå¤šçº¿ç¨‹ä¼˜åŒ–
"""

import os
import pickle
import random
import json
import numpy as np
from itertools import combinations
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import threading
import time
import uuid

# å¤åˆ¶process_ARES_data.pyä¸­çš„merge_with_durationscaleå‡½æ•°
def merge_with_durationscale(times, datas, ratio):
    """
    åŸºäºdurationæ–¹å¼åˆæˆå¤šæ ‡ç­¾æ•°æ®
    
    å‚æ•°:
        times: æ—¶é—´åºåˆ—æ•°ç»„
        datas: æ•°æ®åºåˆ—æ•°ç»„
        ratio: é‡å æ¯”ä¾‹
    
    è¿”å›:
        merged_time: åˆå¹¶åçš„æ—¶é—´åºåˆ—
        merged_data: åˆå¹¶åçš„æ•°æ®åºåˆ—
        None: å¦‚æœåˆå¹¶å¤±è´¥
    """
    # æ£€æŸ¥è¾“å…¥åºåˆ—æ˜¯å¦ä¸ºç©º
    for i, (time, data) in enumerate(zip(times, datas)):
        if len(time) == 0 or len(data) == 0:
            return None
    
    time = times[0]
    data = datas[0]
    
    # æ£€æŸ¥åºåˆ—æ˜¯å¦ä¸ºç©º
    if len(time) == 0 or len(data) == 0:
        return None
    
    split_index = 0
    
    # æ ¹æ®é‡å æ¯”ä¾‹æ‰¾åˆ°åˆ†å‰²ç‚¹
    split_time = np.max(time) * (1 - ratio)
    for i, packet_time in enumerate(time):
        if packet_time >= split_time:
            split_index = i
            break
    
    # å¦‚æœåˆ†å‰²ç‚¹æ˜¯0ï¼Œåˆ™è®¾ç½®ä¸ºåºåˆ—é•¿åº¦çš„ä¸€åŠ
    if split_index == 0:
        split_index = len(time) // 2
    
    # æ·»åŠ ç¬¬ä¸€ä¸ªé¡µé¢çš„å‰åŠéƒ¨åˆ†åˆ°åˆå¹¶åºåˆ—
    merged_time = list(time[:split_index])
    merged_data = list(data[:split_index])
    
    # å‰©ä½™éƒ¨åˆ†ç”¨äºä¸ä¸‹ä¸€ä¸ªé¡µé¢é‡å 
    res_time = time[split_index:]
    res_data = data[split_index:]
    
    # å¦‚æœå‰©ä½™éƒ¨åˆ†ä¸ºç©ºï¼Œåˆ™è®¤ä¸ºæ— æ³•åˆå¹¶
    if len(res_time) == 0 or len(res_data) == 0:
        return None
    
    # å¤„ç†å‰©ä½™çš„é¡µé¢
    for time, data in list(zip(times, datas))[1:]:
        # æ£€æŸ¥åºåˆ—æ˜¯å¦ä¸ºç©º
        if len(time) == 0 or len(data) == 0:
            return None
            
        # è°ƒæ•´æ—¶é—´åºåˆ—ï¼Œä½¿å…¶ä¸å‰ä¸€ä¸ªé¡µé¢çš„å‰©ä½™éƒ¨åˆ†é‡å 
        base_time = res_time[0]  # ä½¿ç”¨å‰©ä½™éƒ¨åˆ†çš„ç¬¬ä¸€ä¸ªæ—¶é—´ç‚¹ä½œä¸ºåŸºå‡†
        time = [(t + base_time) for t in time]
        
        index1 = index2 = 0
        
        # åˆå¹¶ä¸¤ä¸ªåºåˆ—ï¼ŒæŒ‰æ—¶é—´é¡ºåºæ’åº
        while index1 < len(res_time) and index2 < len(time):
            if res_time[index1] <= time[index2]:
                merged_time.append(res_time[index1])
                merged_data.append(res_data[index1])
                index1 += 1
            else:
                merged_time.append(time[index2])
                merged_data.append(data[index2])
                index2 += 1
        
        # å¦‚æœç¬¬ä¸€ä¸ªåºåˆ—æ²¡æœ‰å®Œå…¨éå†ï¼Œç»§ç»­æ·»åŠ å‰©ä½™éƒ¨åˆ†
        while index1 < len(res_time):
            merged_time.append(res_time[index1])
            merged_data.append(res_data[index1])
            index1 += 1
        
        # å¦‚æœç¬¬äºŒä¸ªåºåˆ—æ²¡æœ‰éå†å®Œï¼Œç»§ç»­æ·»åŠ å‰©ä½™éƒ¨åˆ†
        if index2 < len(time):
            remaining_time = time[index2:]
            remaining_data = data[index2:]
            
            # æ ¹æ®é‡å æ¯”ä¾‹æ‰¾åˆ°ç¬¬äºŒä¸ªé¡µé¢çš„åˆ†å‰²ç‚¹
            if len(remaining_time) > 0:
                split_time = remaining_time[-1] * (1 - ratio)
                
                # å¯»æ‰¾åˆ†å‰²ç´¢å¼•
                split_index = len(remaining_time) - 1  # é»˜è®¤ä¸ºæœ€åä¸€ä¸ªç‚¹
                for i, packet_time in enumerate(remaining_time):
                    if packet_time >= split_time:
                        split_index = i
                        break
                
                # æ·»åŠ ç¬¬äºŒä¸ªé¡µé¢çš„ä¸­é—´éƒ¨åˆ†åˆ°åˆå¹¶åºåˆ—
                merged_time.extend(remaining_time[:split_index])
                merged_data.extend(remaining_data[:split_index])
                
                # æ›´æ–°å‰©ä½™éƒ¨åˆ†ç”¨äºä¸‹ä¸€æ¬¡åˆå¹¶
                res_time = remaining_time[split_index:]
                res_data = remaining_data[split_index:]
            else:
                # å¦‚æœæ²¡æœ‰å‰©ä½™éƒ¨åˆ†ï¼Œè®¤ä¸ºæ— æ³•ç»§ç»­åˆå¹¶
                return None
        else:
            # å¦‚æœç¬¬äºŒä¸ªåºåˆ—å·²ç»å…¨éƒ¨åˆå¹¶ï¼Œæ— æ³•ç»§ç»­è¿›è¡Œä¸‹ä¸€è½®åˆå¹¶
            return None
    
    # æ·»åŠ æœ€åä¸€ä¸ªé¡µé¢çš„å‰©ä½™éƒ¨åˆ†
    merged_time.extend(res_time)
    merged_data.extend(res_data)
    
    # ç¡®ä¿åˆå¹¶åçš„åºåˆ—ä¸ä¸ºç©º
    if len(merged_time) == 0 or len(merged_data) == 0:
        return None
    
    return merged_time, merged_data

def load_pickle_sample(file_path):
    """åŠ è½½pickleæ ·æœ¬æ–‡ä»¶"""
    try:
        with open(file_path, 'rb') as f:
            sample = pickle.load(f)
        return sample['time'], sample['data']
    except Exception as e:
        print(f"åŠ è½½æ ·æœ¬å¤±è´¥ {file_path}: {e}")
        return None, None

def merge_3tab_sequences(sample_paths, overlap_ratio=0.1):
    """
    åˆæˆ3æ ‡ç­¾åºåˆ—ï¼Œç¡®ä¿æ ‡ç­¾é¡ºåºä¸æ•°æ®åˆæˆé¡ºåºå¯¹åº”
    
    å‚æ•°:
        sample_paths: æŒ‰æ ‡ç­¾é¡ºåºæ’åˆ—çš„æ ·æœ¬è·¯å¾„åˆ—è¡¨
        overlap_ratio: é‡å æ¯”ä¾‹
    
    è¿”å›:
        merged_time, merged_data: åˆæˆåçš„æ—¶é—´å’Œæ•°æ®åºåˆ—
        None, None: å¦‚æœåˆæˆå¤±è´¥
    """
    # æŒ‰æ ‡ç­¾é¡ºåºåŠ è½½æ ·æœ¬
    times, datas = [], []
    
    for path in sample_paths:  # ç¡®ä¿è·¯å¾„é¡ºåºå¯¹åº”æ ‡ç­¾é¡ºåº
        time_seq, data_seq = load_pickle_sample(path)
        if time_seq is None or data_seq is None:
            return None, None
        times.append(time_seq)
        datas.append(data_seq)
    
    # ä½¿ç”¨durationæ–¹å¼åˆæˆ
    result = merge_with_durationscale(times, datas, ratio=overlap_ratio)
    
    if result is None:
        return None, None
    
    merged_time, merged_data = result
    return np.array(merged_time), np.array(merged_data)

class Tab3DataGenerator:
    """3æ ‡ç­¾æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, support_data_dir, output_dir, overlap_ratio=0.1, samples_per_combination=5):
        self.support_data_dir = support_data_dir
        self.output_dir = output_dir
        self.overlap_ratio = overlap_ratio
        self.samples_per_combination = samples_per_combination
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # å»ºç«‹æ ·æœ¬æ–‡ä»¶ç´¢å¼•
        self.class_samples = {}
        self._build_sample_index()
        
        # JSONç´¢å¼•æ•°æ®
        self.train_index = []
        self.val_index = []
        
        # çº¿ç¨‹é”
        self.lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_generated = 0
        self.failed_attempts = 0
        
    def _build_sample_index(self):
        """å»ºç«‹æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ–‡ä»¶ç´¢å¼•"""
        print("ğŸ” å»ºç«‹æ ·æœ¬æ–‡ä»¶ç´¢å¼•...")
        
        for class_id in range(60):  # 0-59ç±»åˆ«
            class_dir = os.path.join(self.support_data_dir, str(class_id))
            
            if os.path.exists(class_dir):
                pkl_files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith('.pkl')]
                if pkl_files:
                    self.class_samples[class_id] = pkl_files
                    print(f"  ç±»åˆ«{class_id}: {len(pkl_files)}ä¸ªæ ·æœ¬")
                else:
                    print(f"  âš ï¸  ç±»åˆ«{class_id}: æ²¡æœ‰æ‰¾åˆ°pklæ–‡ä»¶")
            else:
                print(f"  âŒ ç±»åˆ«{class_id}: ç›®å½•ä¸å­˜åœ¨")
        
        print(f"âœ… æ ·æœ¬ç´¢å¼•å»ºç«‹å®Œæˆï¼Œå…±{len(self.class_samples)}ä¸ªæœ‰æ•ˆç±»åˆ«")
    
    def generate_combinations(self):
        """ç”Ÿæˆæ‰€æœ‰3ç±»åˆ«ç»„åˆ"""
        available_classes = list(self.class_samples.keys())
        if len(available_classes) < 3:
            raise ValueError(f"å¯ç”¨ç±»åˆ«æ•°é‡ä¸è¶³: {len(available_classes)} < 3")
        
        combinations_list = list(combinations(available_classes, 3))
        print(f"ğŸ“‹ ç”Ÿæˆäº†{len(combinations_list)}ç§3ç±»åˆ«ç»„åˆ")
        return combinations_list
    
    def process_single_combination(self, combination, sample_id):
        """å¤„ç†å•ä¸ªç»„åˆçš„å•æ¬¡é‡‡æ ·"""
        try:
            # æŒ‰é¡ºåºé€‰æ‹©æ ·æœ¬è·¯å¾„ï¼ˆç¡®ä¿æ ‡ç­¾é¡ºåºä¸æ•°æ®é¡ºåºå¯¹åº”ï¼‰
            sample_paths = []
            for class_id in combination:  # combinationå·²ç»æ˜¯æ’åºçš„
                available_samples = self.class_samples[class_id]
                selected_sample = random.choice(available_samples)
                sample_paths.append(selected_sample)
            
            # åˆæˆæ•°æ®
            merged_time, merged_data = merge_3tab_sequences(sample_paths, self.overlap_ratio)
            
            if merged_time is None or merged_data is None:
                return None
            
            # ç”Ÿæˆæ–‡ä»¶åï¼šç±»åˆ«1_ç±»åˆ«2_ç±»åˆ«3_éšæœºID.pkl
            labels_str = "_".join(map(str, combination))
            random_id = str(uuid.uuid4())[:8]
            filename = f"{labels_str}_{random_id}.pkl"
            
            # ä¿å­˜æ•°æ®
            sample_data = {
                'time': merged_time,
                'data': merged_data,
                'labels': list(combination),  # ä¿æŒé¡ºåº
                'source_files': sample_paths
            }
            
            file_path = os.path.join(self.output_dir, filename)
            with open(file_path, 'wb') as f:
                pickle.dump(sample_data, f)
            
            # åˆ›å»ºç´¢å¼•æ¡ç›®
            index_entry = {
                'filename': filename,
                'labels': list(combination),
                'data_length': len(merged_data),
                'time_range': [float(merged_time[0]), float(merged_time[-1])]
            }
            
            # æŒ‰4:1åˆ†é…è®­ç»ƒå’ŒéªŒè¯
            if sample_id < 4:  # å‰4ä¸ªä½œä¸ºè®­ç»ƒé›†
                dataset_type = 'train'
            else:  # ç¬¬5ä¸ªä½œä¸ºéªŒè¯é›†
                dataset_type = 'val'
            
            return index_entry, dataset_type
            
        except Exception as e:
            print(f"å¤„ç†ç»„åˆ{combination}å¤±è´¥: {e}")
            return None
    
    def process_combination_batch(self, combination):
        """å¤„ç†å•ä¸ªç»„åˆçš„æ‰€æœ‰é‡‡æ ·ï¼ˆ5æ¬¡ï¼‰"""
        results = []
        
        for sample_id in range(self.samples_per_combination):
            result = self.process_single_combination(combination, sample_id)
            if result is not None:
                results.append(result)
            else:
                with self.lock:
                    self.failed_attempts += 1
        
        # æ›´æ–°ç´¢å¼•
        with self.lock:
            for index_entry, dataset_type in results:
                if dataset_type == 'train':
                    self.train_index.append(index_entry)
                else:
                    self.val_index.append(index_entry)
            
            self.total_generated += len(results)
        
        return len(results)
    
    def generate_all_data(self, num_threads=4):
        """ç”Ÿæˆæ‰€æœ‰å¤šæ ‡ç­¾æ•°æ®"""
        print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ3æ ‡ç­¾å¤šæ ‡ç­¾æ•°æ®...")
        print(f"  - é‡å æ¯”ä¾‹: {self.overlap_ratio}")
        print(f"  - æ¯ç»„åˆæ ·æœ¬æ•°: {self.samples_per_combination}")
        print(f"  - çº¿ç¨‹æ•°: {num_threads}")
        
        # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
        combinations_list = self.generate_combinations()
        total_expected = len(combinations_list) * self.samples_per_combination
        
        print(f"  - é¢„æœŸç”Ÿæˆæ ·æœ¬æ€»æ•°: {total_expected}")
        
        # å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = []
            for combination in combinations_list:
                future = executor.submit(self.process_combination_batch, combination)
                futures.append((combination, future))
            
            # æ”¶é›†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(combinations_list), desc="å¤„ç†ç»„åˆ") as pbar:
                for combination, future in futures:
                    try:
                        generated_count = future.result(timeout=60)  # 60ç§’è¶…æ—¶
                        pbar.set_postfix({
                            'å·²ç”Ÿæˆ': self.total_generated,
                            'å¤±è´¥': self.failed_attempts,
                            'å½“å‰ç»„åˆ': f"{combination[0]}-{combination[1]}-{combination[2]}"
                        })
                        pbar.update(1)
                    except Exception as e:
                        print(f"ç»„åˆ{combination}å¤„ç†è¶…æ—¶æˆ–å‡ºé”™: {e}")
                        pbar.update(1)
        
        # ä¿å­˜JSONç´¢å¼•
        self.save_json_indices()
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report()
    
    def save_json_indices(self):
        """ä¿å­˜JSONç´¢å¼•æ–‡ä»¶"""
        print(f"\nğŸ’¾ ä¿å­˜JSONç´¢å¼•æ–‡ä»¶...")
        
        # æå–æ–‡ä»¶ååˆ—è¡¨ï¼ˆå…¼å®¹ç°æœ‰æ•°æ®åŠ è½½å™¨æ ¼å¼ï¼‰
        train_filenames = [entry['filename'] for entry in self.train_index]
        val_filenames = [entry['filename'] for entry in self.val_index]
        
        # ä¿å­˜è®­ç»ƒé›†ç´¢å¼•
        train_json_path = os.path.join(os.path.dirname(self.output_dir), "3tab_train.json")
        with open(train_json_path, 'w', encoding='utf-8') as f:
            json.dump(train_filenames, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜éªŒè¯é›†ç´¢å¼•
        val_json_path = os.path.join(os.path.dirname(self.output_dir), "3tab_val.json")
        with open(val_json_path, 'w', encoding='utf-8') as f:
            json.dump(val_filenames, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ä¿å­˜è¯¦ç»†çš„å…ƒæ•°æ®ç´¢å¼•ï¼ˆç”¨äºåˆ†æï¼‰
        detailed_train_path = os.path.join(os.path.dirname(self.output_dir), "3tab_train_detailed.json")
        with open(detailed_train_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_index, f, ensure_ascii=False, indent=2)
            
        detailed_val_path = os.path.join(os.path.dirname(self.output_dir), "3tab_val_detailed.json")
        with open(detailed_val_path, 'w', encoding='utf-8') as f:
            json.dump(self.val_index, f, ensure_ascii=False, indent=2)
        
        print(f"  âœ… è®­ç»ƒé›†ç´¢å¼•: {train_json_path} ({len(train_filenames)}æ¡)")
        print(f"  âœ… éªŒè¯é›†ç´¢å¼•: {val_json_path} ({len(val_filenames)}æ¡)")
        print(f"  ğŸ“‹ è¯¦ç»†å…ƒæ•°æ®: {detailed_train_path}, {detailed_val_path}")
    
    def generate_report(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        print(f"\nğŸ“Š æ•°æ®ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š:")
        print(f"  - æ€»ç”Ÿæˆæ ·æœ¬: {self.total_generated}")
        print(f"  - è®­ç»ƒé›†æ ·æœ¬: {len(self.train_index)}")
        print(f"  - éªŒè¯é›†æ ·æœ¬: {len(self.val_index)}")
        print(f"  - å¤±è´¥å°è¯•: {self.failed_attempts}")
        print(f"  - æˆåŠŸç‡: {self.total_generated/(self.total_generated+self.failed_attempts)*100:.2f}%")
        
        # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
        if len(self.train_index) > 0:
            train_lengths = [entry['data_length'] for entry in self.train_index]
            print(f"  - è®­ç»ƒé›†æ•°æ®é•¿åº¦: {np.min(train_lengths)}-{np.max(train_lengths)} (å¹³å‡{np.mean(train_lengths):.0f})")
        
        if len(self.val_index) > 0:
            val_lengths = [entry['data_length'] for entry in self.val_index]
            print(f"  - éªŒè¯é›†æ•°æ®é•¿åº¦: {np.min(val_lengths)}-{np.max(val_lengths)} (å¹³å‡{np.mean(val_lengths):.0f})")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    support_data_dir = "datasets/3tab_exp/base_train/support_data"
    output_dir = "datasets/3tab_exp/base_train/query_data"
    overlap_ratio = 0.1
    samples_per_combination = 5
    num_threads = 4
    
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)
    
    print("ğŸ¯ 3æ ‡ç­¾å¤šæ ‡ç­¾æ•°æ®ç”Ÿæˆå™¨")
    print("="*50)
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = Tab3DataGenerator(
        support_data_dir=support_data_dir,
        output_dir=output_dir,
        overlap_ratio=overlap_ratio,
        samples_per_combination=samples_per_combination
    )
    
    # ç”Ÿæˆæ‰€æœ‰æ•°æ®
    start_time = time.time()
    generator.generate_all_data(num_threads=num_threads)
    end_time = time.time()
    
    print(f"\nğŸ‰ æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print(f"â±ï¸  æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")

if __name__ == "__main__":
    main() 