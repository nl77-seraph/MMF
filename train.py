"""
Base Classè®­ç»ƒä¸»ç¨‹åº
å®ç°åŸºäºEpochçš„å¤šæ ‡ç­¾ç½‘ç«™æŒ‡çº¹è¯†åˆ«è®­ç»ƒ
ç‰¹åˆ«å¤„ç†ä¸¥é‡çš„ç±»åˆ«ä¸å‡è¡¡é—®é¢˜ï¼ˆ3:57æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ï¼‰
æ”¯æŒå•GPUå’Œå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import os
import sys
import time
import json
from datetime import datetime
import argparse
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from data.meta_traffic_dataloader import MetaTrafficDataLoader
from models.feature_extractors import MultiMetaFingerNet
from utils.metrics import MultiLabelMetrics
from utils.loss_functions import WeightedBCELoss, FocalLoss
from utils.model_manager import ModelManager


def setup_distributed_training(rank, world_size, config):
    """
    è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    
    Args:
        rank: å½“å‰è¿›ç¨‹çš„rank
        world_size: æ€»è¿›ç¨‹æ•°
        config: é…ç½®å­—å…¸
    """
    # è®¾ç½®CUDAè®¾å¤‡
    torch.cuda.set_device(rank)
    
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    os.environ['MASTER_ADDR'] = config.get('master_addr', 'localhost')
    os.environ['MASTER_PORT'] = config.get('master_port', '12355')
    
    dist.init_process_group(
        backend=config.get('dist_backend', 'nccl'),
        init_method='env://',
        world_size=world_size,
        rank=rank
    )
    
    print(f"ğŸš€ åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–å®Œæˆ - Rank: {rank}/{world_size}")


def cleanup_distributed_training():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def is_main_process():
    """æ£€æŸ¥æ˜¯å¦ä¸ºä¸»è¿›ç¨‹"""
    return not dist.is_initialized() or dist.get_rank() == 0


class BaseClassTrainer:
    """
    Base Classè®­ç»ƒå™¨
    ä¸“é—¨å¤„ç†å¤šæ ‡ç­¾åˆ†ç±»çš„è®­ç»ƒï¼Œé‡ç‚¹è§£å†³ç±»åˆ«ä¸å‡è¡¡é—®é¢˜
    æ”¯æŒå•GPUå’Œå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
    """
    
    def __init__(self, config, rank=None, world_size=None):
        self.config = config
        self.rank = rank if rank is not None else 0
        self.world_size = world_size if world_size is not None else 1
        self.is_distributed = world_size is not None and world_size > 1
        
        # è®¾ç½®è®¾å¤‡
        if self.is_distributed:
            self.device = torch.device(f'cuda:{rank}')
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
        self.model = None
        self.train_loader = None
        self.val_loader = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_map = 0.0
        self.train_losses = []
        self.val_metrics = []
        
        # æ—¥å¿—å’Œå¯è§†åŒ–ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        self.writer = None
        self.model_manager = None
        
        # ç±»åˆ«ä¸å‡è¡¡å¤„ç†
        self.pos_weight = None
        self.class_weights = None
        
        if is_main_process():
            print(f"ğŸš€ BaseClassTraineråˆå§‹åŒ–")
            print(f"  - æ¨¡å¼: {'åˆ†å¸ƒå¼è®­ç»ƒ' if self.is_distributed else 'å•GPUè®­ç»ƒ'}")
            if self.is_distributed:
                print(f"  - Rank: {self.rank}/{self.world_size}")
            print(f"  - è®¾å¤‡: {self.device}")
            print(f"  - ç±»åˆ«æ•°: {config['num_classes']}")
            print(f"  - æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: ~{config['positive_ratio']}:1 (ä¸¥é‡ä¸å‡è¡¡)")
    
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒåˆ†å¸ƒå¼é‡‡æ ·ï¼‰"""
        if is_main_process():
            print("\nğŸ“¦ è®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼ˆéšæœºé‡‡æ ·æ¨¡å¼ï¼‰
        train_loader_base = MetaTrafficDataLoader(
            query_json_path=self.config['train_query_json'],
            query_files_dir=self.config['train_query_dir'],
            support_root_dir=self.config['support_root_dir'],
            activated_classes=list(range(self.config['num_classes'])),
            target_length=self.config['sequence_length'],
            shots_per_class=self.config['shots_per_class'],
            batch_size=self.config['batch_size'],
            shuffle=not self.is_distributed,  # åˆ†å¸ƒå¼æ¨¡å¼ä¸‹ç”±DistributedSampleræ§åˆ¶
            num_workers=self.config['num_workers'],
            random_sampling=True  # è®­ç»ƒä½¿ç”¨éšæœºé‡‡æ ·
        )
        
        # å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŒ…è£…æ•°æ®åŠ è½½å™¨
        if self.is_distributed:
            # åˆ†å¸ƒå¼é‡‡æ ·å™¨
            self.train_sampler = DistributedSampler(
                train_loader_base.query_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )
            
            # é‡æ–°åˆ›å»ºDataLoader with DistributedSampler
            from torch.utils.data import DataLoader
            train_loader_base.query_loader = DataLoader(
                train_loader_base.query_dataset,
                batch_size=self.config['batch_size'],
                sampler=self.train_sampler,
                num_workers=self.config['num_workers'],
                collate_fn=train_loader_base._query_collate_fn
            )
        else:
            self.train_sampler = None
        
        self.train_loader = train_loader_base
        
        # éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆå›ºå®šé‡‡æ ·æ¨¡å¼ï¼‰
        # éªŒè¯æ•°æ®ä¸éœ€è¦åˆ†å¸ƒå¼é‡‡æ ·ï¼Œæ¯ä¸ªè¿›ç¨‹éªŒè¯ç›¸åŒçš„æ•°æ®
        self.val_loader = MetaTrafficDataLoader(
            query_json_path=self.config['val_query_json'],
            query_files_dir=self.config['val_query_dir'],
            support_root_dir=self.config['support_root_dir'],
            activated_classes=list(range(self.config['num_classes'])),
            target_length=self.config['sequence_length'],
            shots_per_class=self.config['shots_per_class'],
            batch_size=self.config['val_batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            random_sampling=True  # éªŒè¯ä½¿ç”¨å›ºå®šé‡‡æ ·
        )
        
        if is_main_process():
            print(f"  âœ… è®­ç»ƒé›†: {len(self.train_loader)} batches")
            print(f"  âœ… éªŒè¯é›†: {len(self.val_loader)} batches")
            if self.is_distributed:
                print(f"  âœ… åˆ†å¸ƒå¼é‡‡æ ·å™¨: å·²å¯ç”¨")
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹ï¼ˆæ”¯æŒDDPåŒ…è£…ï¼‰"""
        if is_main_process():
            print("\nğŸ§  è®¾ç½®ç½‘ç»œæ¨¡å‹...")
        
        self.model = MultiMetaFingerNet(
            num_classes=self.config['num_classes'],
            dropout=self.config['dropout'],
            support_blocks=self.config['support_blocks'],
            classification_method=self.config['classification_method'],
            unified_threshold=self.config['unified_threshold']
        ).to(self.device)
        
        # å¦‚æœæ˜¯åˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŒ…è£…ä¸ºDDP
        if self.is_distributed:
            self.model = DDP(
                self.model,
                device_ids=[self.rank],
                output_device=self.rank,
                find_unused_parameters=False  # æå‡æ€§èƒ½
            )
            if is_main_process():
                print(f"  âœ… DDPæ¨¡å‹åŒ…è£…å®Œæˆ - Device: {self.rank}")
        
        # è®¡ç®—æ¨¡å‹å‚æ•°é‡ï¼ˆä»…ä¸»è¿›ç¨‹è¾“å‡ºï¼‰
        if is_main_process():
            model_for_count = self.model.module if self.is_distributed else self.model
            total_params = sum(p.numel() for p in model_for_count.parameters())
            trainable_params = sum(p.numel() for p in model_for_count.parameters() if p.requires_grad)
            
            print(f"  âœ… æ¨¡å‹å‚æ•°: {total_params:,} æ€»é‡, {trainable_params:,} å¯è®­ç»ƒ")
            print(f"  âœ… æ¨¡å‹å·²ç§»è‡³: {self.device}")
    
    def setup_loss_function(self):
        """è®¾ç½®æŸå¤±å‡½æ•°ï¼Œé‡ç‚¹å¤„ç†ç±»åˆ«ä¸å‡è¡¡"""
        if is_main_process():
            print("\nâš–ï¸ è®¾ç½®æŸå¤±å‡½æ•°ï¼ˆå¤„ç†ç±»åˆ«ä¸å‡è¡¡ï¼‰...")
        
        # è®¡ç®—æ­£è´Ÿæ ·æœ¬æƒé‡
        positive_ratio = self.config['positive_ratio']  # 3:57çš„æ­£è´Ÿæ¯”ä¾‹
        pos_weight = torch.tensor([positive_ratio] * self.config['num_classes']).to(self.device)
        self.pos_weight = pos_weight
        
        loss_type = self.config['loss_type']
        
        if loss_type == 'weighted_bce':
            self.criterion = WeightedBCELoss(pos_weight=pos_weight)
            if is_main_process():
                print(f"  âœ… ä½¿ç”¨Weighted BCE Loss, æ­£æ ·æœ¬æƒé‡: {positive_ratio:.1f}")
                
        elif loss_type == 'focal':
            self.criterion = FocalLoss(
                alpha=self.config['focal_alpha'],
                gamma=self.config['focal_gamma'],
                pos_weight=pos_weight
            )
            if is_main_process():
                print(f"  âœ… ä½¿ç”¨Focal Loss, alpha={self.config['focal_alpha']}, gamma={self.config['focal_gamma']}")
                
        else:
            # æ ‡å‡†BCEä½œä¸ºåŸºå‡†
            self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
            if is_main_process():
                print(f"  âœ… ä½¿ç”¨æ ‡å‡†BCE Loss with pos_weight")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if is_main_process():
            print("\nğŸ¯ è®¾ç½®ä¼˜åŒ–å™¨...")
        
        # è·å–æ¨¡å‹å‚æ•°ï¼ˆè€ƒè™‘DDPåŒ…è£…ï¼‰
        model_params = self.model.module.parameters() if self.is_distributed else self.model.parameters()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        if self.config['optimizer'] == 'adam':
            self.optimizer = optim.Adam(
                model_params,
                lr=self.config['learning_rate'],
                weight_decay=self.config['weight_decay']
            )
        elif self.config['optimizer'] == 'sgd':
            self.optimizer = optim.SGD(
                model_params,
                lr=self.config['learning_rate'],
                momentum=self.config['momentum'],
                weight_decay=self.config['weight_decay']
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config['scheduler'] == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['num_epochs'],
                eta_min=self.config['min_lr']
            )
        elif self.config['scheduler'] == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config['step_size'],
                gamma=self.config['gamma']
            )
        
        if is_main_process():
            print(f"  âœ… ä¼˜åŒ–å™¨: {self.config['optimizer']}")
            print(f"  âœ… å­¦ä¹ ç‡: {self.config['learning_rate']}")
            print(f"  âœ… è°ƒåº¦å™¨: {self.config['scheduler']}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—å’Œæ¨¡å‹ç®¡ç†ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰"""
        if not is_main_process():
            return
            
        print("\nğŸ“Š è®¾ç½®æ—¥å¿—ç³»ç»Ÿ...")
        
        # åˆ›å»ºå®éªŒç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mode_suffix = "_ddp" if self.is_distributed else "_single"
        exp_name = f"base_training_{timestamp}{mode_suffix}"
        self.exp_dir = os.path.join(self.config['output_dir'], exp_name)
        os.makedirs(self.exp_dir, exist_ok=True)
        
        # TensorBoard
        log_dir = os.path.join(self.exp_dir, 'logs')
        self.writer = SummaryWriter(log_dir)
        
        # æ¨¡å‹ç®¡ç†å™¨
        checkpoint_dir = os.path.join(self.exp_dir, 'checkpoints')
        self.model_manager = ModelManager(checkpoint_dir)
        
        print(f"  âœ… å®éªŒç›®å½•: {self.exp_dir}")
        print(f"  âœ… TensorBoardæ—¥å¿—: {log_dir}")
        print(f"  âœ… Checkpointç›®å½•: {checkpoint_dir}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼‰"""
        self.model.train()
        
        # åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦è®¾ç½®epochç”¨äºshuffle
        if self.is_distributed and self.train_sampler:
            self.train_sampler.set_epoch(epoch)
        
        train_losses = []
        all_train_logits = []
        all_train_labels = []
        
        # è®­ç»ƒå¾ªç¯
        for batch_idx, batch in enumerate(self.train_loader):
            query_data, support_data, support_masks, batch_info = batch
            
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            query_data = query_data.to(self.device)
            support_data = support_data.to(self.device)
            support_masks = support_masks.to(self.device)
            query_labels = batch_info['query_labels'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            results = self.model(query_data, support_data, support_masks)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(results['logits'], query_labels.float())
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.get('grad_clip', 0) > 0:
                model_params = self.model.module.parameters() if self.is_distributed else self.model.parameters()
                torch.nn.utils.clip_grad_norm_(model_params, self.config['grad_clip'])
            
            self.optimizer.step()
            
            # è®°å½•æŸå¤±å’Œé¢„æµ‹ï¼ˆç”¨äºè®­ç»ƒepochè¯„ä¼°ï¼‰
            train_losses.append(loss.item())
            all_train_logits.append(results['logits'].detach().cpu())
            all_train_labels.append(query_labels.detach().cpu())
            
            # åœ¨è®­ç»ƒçš„10%æ—¶æ˜¾ç¤ºé¢„æµ‹æ ·æœ¬ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and batch_idx == len(self.train_loader) // 10:
                self.print_prediction_samples(results['logits'], query_labels, epoch)
            
            # è®°å½•æ‰¹çº§åˆ«æŒ‡æ ‡åˆ°TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process() and batch_idx % 10 == 0:
                step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Train/BatchLoss', loss.item(), step)
                self.writer.add_scalar('Train/LearningRate', self.optimizer.param_groups[0]['lr'], step)
                
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                model_params = self.model.module.parameters() if self.is_distributed else self.model.parameters()
                grad_norm = sum(p.grad.norm().item() ** 2 for p in model_params if p.grad is not None) ** 0.5
                self.writer.add_scalar('Train/GradientNorm', grad_norm, step)
        
        # è®¡ç®—å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = np.mean(train_losses)
        
        # è®¡ç®—è®­ç»ƒepochçš„æ•´ä½“è¯„ä¼°æŒ‡æ ‡
        all_train_logits = torch.cat(all_train_logits, dim=0)
        all_train_labels = torch.cat(all_train_labels, dim=0)
        train_metrics = self.evaluate_training_epoch(all_train_logits, all_train_labels)
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            self.log_training_metrics(train_metrics, avg_train_loss, epoch)
        
        return avg_train_loss, train_metrics
    
    def evaluate_training_epoch(self, all_logits, all_labels):
        """å¯¹æ•´ä¸ªè®­ç»ƒepochè¿›è¡Œè¯„ä¼°"""
        return MultiLabelMetrics.compute_metrics(all_logits, all_labels)
    
    def log_training_metrics(self, train_metrics, train_loss, epoch):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰"""
        if not is_main_process():
            return
            
        # è®°å½•è®­ç»ƒæŸå¤±å’ŒæŒ‡æ ‡ï¼ˆä½¿ç”¨'Train/'å‰ç¼€ï¼‰
        self.writer.add_scalar('Train/EpochLoss', train_loss, epoch)
        self.writer.add_scalar('Train/mAP', train_metrics['mAP'], epoch)
        self.writer.add_scalar('Train/Precision', train_metrics['avg_precision'], epoch)
        self.writer.add_scalar('Train/Recall', train_metrics['avg_recall'], epoch)
        self.writer.add_scalar('Train/F1', train_metrics['avg_f1'], epoch)
        self.writer.add_scalar('Train/PositiveRate', train_metrics['positive_rate'], epoch)
        self.writer.add_scalar('Train/PredictionRate', train_metrics['prediction_rate'], epoch)
        
        # è®°å½•microå¹³å‡æŒ‡æ ‡
        self.writer.add_scalar('Train/mAP_micro', train_metrics['mAP_micro'], epoch)
        self.writer.add_scalar('Train/Precision_micro', train_metrics['precision_micro'], epoch)
        self.writer.add_scalar('Train/Recall_micro', train_metrics['recall_micro'], epoch)
        self.writer.add_scalar('Train/F1_micro', train_metrics['f1_micro'], epoch)
    
    def print_prediction_samples(self, logits, labels, epoch):
        """æ‰“å°é¢„æµ‹æ ·æœ¬ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰"""
        if not is_main_process():
            return
            
        print(f"\nğŸ“‹ Epoch {epoch+1} è®­ç»ƒæ ·æœ¬é¢„æµ‹å±•ç¤º (å‰5ä¸ª):")
        
        # è½¬æ¢ä¸ºé¢„æµ‹æ ‡ç­¾
        predictions = torch.sigmoid(logits) > 0.5
        
        for i in range(min(5, logits.size(0))):
            # è·å–é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
            pred_indices = torch.where(predictions[i])[0].cpu().numpy()
            true_indices = torch.where(labels[i] > 0.5)[0].cpu().numpy()
            
            # è®¡ç®—åŒ¹é…æƒ…å†µ
            correct_preds = set(pred_indices) & set(true_indices)
            
            print(f"  æ ·æœ¬{i+1}: é¢„æµ‹={pred_indices.tolist()}, çœŸå®={true_indices.tolist()}, åŒ¹é…={len(correct_preds)}/{len(true_indices)}")
    
    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        val_losses = []
        all_logits = []
        all_labels = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                query_data, support_data, support_masks, batch_info = batch
                
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                query_data = query_data.to(self.device)
                support_data = support_data.to(self.device)
                support_masks = support_masks.to(self.device)
                query_labels = batch_info['query_labels'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                results = self.model(query_data, support_data, support_masks)
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(results['logits'], query_labels.float())
                val_losses.append(loss.item())
                
                # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾
                all_logits.append(results['logits'].cpu())
                all_labels.append(query_labels.cpu())
        
        # è®¡ç®—éªŒè¯æŒ‡æ ‡
        all_logits = torch.cat(all_logits, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        metrics = MultiLabelMetrics.compute_metrics(all_logits, all_labels)
        avg_val_loss = np.mean(val_losses)
        
        # è®°å½•éªŒè¯æŒ‡æ ‡åˆ°TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        if is_main_process():
            self.log_validation_metrics(metrics, avg_val_loss, epoch)
        
        return avg_val_loss, metrics
    
    def log_validation_metrics(self, val_metrics, val_loss, epoch):
        """è®°å½•éªŒè¯æŒ‡æ ‡åˆ°TensorBoardï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰"""
        if not is_main_process():
            return
            
        # è®°å½•éªŒè¯æŸå¤±å’ŒæŒ‡æ ‡ï¼ˆä½¿ç”¨'Val/'å‰ç¼€ï¼‰
        self.writer.add_scalar('Val/EpochLoss', val_loss, epoch)
        self.writer.add_scalar('Val/mAP', val_metrics['mAP'], epoch)
        self.writer.add_scalar('Val/Precision', val_metrics['avg_precision'], epoch)
        self.writer.add_scalar('Val/Recall', val_metrics['avg_recall'], epoch)
        self.writer.add_scalar('Val/F1', val_metrics['avg_f1'], epoch)
        self.writer.add_scalar('Val/PositiveRate', val_metrics['positive_rate'], epoch)
        self.writer.add_scalar('Val/PredictionRate', val_metrics['prediction_rate'], epoch)
        
        # è®°å½•microå¹³å‡æŒ‡æ ‡
        self.writer.add_scalar('Val/mAP_micro', val_metrics['mAP_micro'], epoch)
        self.writer.add_scalar('Val/Precision_micro', val_metrics['precision_micro'], epoch)
        self.writer.add_scalar('Val/Recall_micro', val_metrics['recall_micro'], epoch)
        self.writer.add_scalar('Val/F1_micro', val_metrics['f1_micro'], epoch)
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼‰"""
        if is_main_process():
            print("\nğŸš€ å¼€å§‹Base Classè®­ç»ƒ...")
            print(f"  - è®­ç»ƒæ¨¡å¼: {'åˆ†å¸ƒå¼' if self.is_distributed else 'å•GPU'}")
            if self.is_distributed:
                print(f"  - GPUæ•°é‡: {self.world_size}")
            print(f"  - è®­ç»ƒè½®æ•°: {self.config['num_epochs']}")
            print(f"  - æ‰¹å¤§å°: {self.config['batch_size']}")
            print(f"  - ç±»åˆ«ä¸å‡è¡¡æ¯”ä¾‹: 1:{self.config['positive_ratio']}")
            print(f"  - éªŒè¯é¢‘ç‡: æ¯ä¸ªepochï¼ˆå¢å¼ºç›‘æ§ï¼‰")
        
        for epoch in range(self.config['num_epochs']):
            self.current_epoch = epoch
            
            # è®­ç»ƒé˜¶æ®µï¼ˆæ–°å¢è®­ç»ƒæŒ‡æ ‡è¯„ä¼°ï¼‰
            train_loss, train_metrics = self.train_epoch(epoch)
            self.train_losses.append(train_loss)
            
            # æ¯ä¸ªepochéƒ½è¿›è¡ŒéªŒè¯ï¼ˆæ¢å¤ç»†ç²’åº¦ç›‘æ§ï¼‰
            val_loss, val_metrics = self.validate_epoch(epoch)
            self.val_metrics.append(val_metrics)
            
            # æ‰“å°è®­ç»ƒå’ŒéªŒè¯ä¿¡æ¯ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
            if is_main_process():
                print(f"\nEpoch {epoch+1}/{self.config['num_epochs']} [è®­ç»ƒ+éªŒè¯ç»“æœ]:")
                print(f"  ğŸ“ˆ Train - Loss:{train_loss:.4f}, mAP:{train_metrics['mAP']:.4f}, P:{train_metrics['avg_precision']:.4f}, R:{train_metrics['avg_recall']:.4f}")
                print(f"  ğŸ“Š Val   - Loss:{val_loss:.4f}, mAP:{val_metrics['mAP']:.4f}, P:{val_metrics['avg_precision']:.4f}, R:{val_metrics['avg_recall']:.4f}")
                
                # æ¨¡å‹ä¿å­˜ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
                is_best = val_metrics['mAP'] > self.best_map
                if is_best:
                    self.best_map = val_metrics['mAP']
                    print(f"  ğŸ‰ æ–°æœ€ä½³mAP: {self.best_map:.4f}")
                
                # ä¿å­˜checkpointï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
                model_to_save = self.model.module if self.is_distributed else self.model
                self.model_manager.save_checkpoint(
                    model=model_to_save,
                    optimizer=self.optimizer,
                    scheduler=self.scheduler,
                    epoch=epoch,
                    metrics=val_metrics,
                    is_best=is_best
                )
            
            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼ˆç¡®ä¿æ‰€æœ‰GPUå®Œæˆå½“å‰epochå†ç»§ç»­ï¼‰
            if self.is_distributed:
                dist.barrier()
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
        
        if is_main_process():
            print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³mAP: {self.best_map:.4f}")
            if self.writer:
                self.writer.close()


def run_distributed_training(rank, world_size, config):
    """
    åˆ†å¸ƒå¼è®­ç»ƒçš„å·¥ä½œå‡½æ•°
    
    Args:
        rank: å½“å‰è¿›ç¨‹çš„rank
        world_size: æ€»è¿›ç¨‹æ•°
        config: é…ç½®å­—å…¸
    """
    try:
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        setup_distributed_training(rank, world_size, config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = BaseClassTrainer(config, rank=rank, world_size=world_size)
        
        # è®¾ç½®æ‰€æœ‰ç»„ä»¶
        trainer.setup_data_loaders()
        trainer.setup_model()
        trainer.setup_loss_function()
        trainer.setup_optimizer()
        trainer.setup_logging()
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
    except Exception as e:
        print(f"âŒ Rank {rank} è®­ç»ƒå¤±è´¥: {e}")
        raise e
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed_training()


def get_default_config():
    """è·å–é»˜è®¤é…ç½®"""
    return {
        # æ•°æ®é…ç½®
        'train_query_json': 'datasets/3tab_exp/base_train/3tab_train.json',
        'train_query_dir': 'datasets/3tab_exp/base_train/query_data',
        'val_query_json': 'datasets/3tab_exp/base_train/3tab_val.json',
        'val_query_dir': 'datasets/3tab_exp/base_train/query_data',
        'support_root_dir': 'datasets/3tab_exp/base_train/support_data',
        
        # æ¨¡å‹é…ç½®
        'num_classes': 60,
        'sequence_length': 30000,
        'shots_per_class': 1,
        'support_blocks': 0,
        'dropout': 0.15,
        
        # åˆ†ç±»å¤´é…ç½®
        'classification_method': 'binary',  # 'binary' 
        'unified_threshold': 0.4,  # unifiedæ–¹æ³•çš„é˜ˆå€¼
        
        # è®­ç»ƒé…ç½®
        'num_epochs': 100,
        'batch_size': 8,
        'val_batch_size': 8,
        'num_workers': 0,
        
        # ä¼˜åŒ–å™¨é…ç½®
        'optimizer': 'adam',
        'learning_rate': 5e-5,
        'weight_decay': 1e-4,
        'momentum': 0.9,
        'grad_clip': 1.0,
        
        # å­¦ä¹ ç‡è°ƒåº¦
        'scheduler': 'cosine',
        'step_size': 30,
        'gamma': 0.1,
        'min_lr': 1e-6,
        
        # æŸå¤±å‡½æ•°é…ç½®ï¼ˆå¤„ç†ç±»åˆ«ä¸å‡è¡¡ï¼‰
        'loss_type': 'weighted_bce',  # 'weighted_bce', 'focal', 'bce'
        'positive_ratio': 19.0,  # 57/3 â‰ˆ 19ï¼Œè´Ÿæ­£æ ·æœ¬æ¯”ä¾‹
        'focal_alpha': 0.25,
        'focal_gamma': 2.0,
        
        # åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
        'use_distributed': False,      # æ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
        'gpus': [0],                   # ä½¿ç”¨çš„GPUåˆ—è¡¨
        'dist_backend': 'nccl',        # åˆ†å¸ƒå¼åç«¯
        'master_addr': 'localhost',    # ä¸»èŠ‚ç‚¹åœ°å€
        'master_port': '12355',        # ä¸»èŠ‚ç‚¹ç«¯å£
        
        # è¾“å‡ºé…ç½®
        'output_dir': './experiments',
        'save_interval': 10,
    }


def validate_gpu_config(config):
    """
    éªŒè¯GPUé…ç½®çš„åˆæ³•æ€§
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        validated_config: éªŒè¯åçš„é…ç½®
        is_valid: æ˜¯å¦æœ‰æ•ˆ
        error_msg: é”™è¯¯ä¿¡æ¯
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰CUDAæ”¯æŒ
        if not torch.cuda.is_available():
            if config.get('use_distributed', False):
                return config, False, "âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUè®­ç»ƒ"
            else:
                print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
                config['use_distributed'] = False
                return config, True, ""
        
        # æ£€æŸ¥GPUæ•°é‡
        available_gpus = torch.cuda.device_count()
        print(f"ğŸ” æ£€æµ‹åˆ° {available_gpus} ä¸ªGPU")
        
        # éªŒè¯è¯·æ±‚çš„GPUæ˜¯å¦å­˜åœ¨
        requested_gpus = config.get('gpus', [0])
        if not isinstance(requested_gpus, list):
            return config, False, "âŒ 'gpus' é…ç½®å¿…é¡»æ˜¯åˆ—è¡¨"
        
        for gpu_id in requested_gpus:
            if gpu_id >= available_gpus:
                return config, False, f"âŒ GPU {gpu_id} ä¸å­˜åœ¨ï¼ˆåªæœ‰ {available_gpus} ä¸ªGPUï¼‰"
        
        # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å¼
        num_gpus = len(requested_gpus)
        if num_gpus > 1:
            config['use_distributed'] = True
            print(f"âœ… å°†ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ: {requested_gpus}")
        else:
            config['use_distributed'] = False
            print(f"âœ… å°†ä½¿ç”¨å•GPUè®­ç»ƒ: GPU {requested_gpus[0]}")
        
        return config, True, ""
        
    except Exception as e:
        return config, False, f"âŒ GPUé…ç½®éªŒè¯å¤±è´¥: {e}"


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Base Class Training with Multi-GPU Support')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_epochs', type=int, default=20, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--loss_type', type=str, default='weighted_bce', 
                        choices=['weighted_bce', 'focal', 'bce'], help='æŸå¤±å‡½æ•°ç±»å‹')
    parser.add_argument('--classification_method', type=str, default='binary',
                        choices=['binary'], help='åˆ†ç±»æ–¹æ³•ï¼šbinaryæˆ–unified')
    parser.add_argument('--unified_threshold', type=float, default=0.4, 
                        help='unifiedæ–¹æ³•çš„åˆ†ç±»é˜ˆå€¼')
    
    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    parser.add_argument('--use_distributed', action='store_true', 
                        help='æ˜¯å¦å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ')
    parser.add_argument('--gpus', nargs='+', type=int, default=[0],
                        help='ä½¿ç”¨çš„GPUåˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š--gpus 0 1 2 3')
    parser.add_argument('--master_addr', type=str, default='localhost',
                        help='ä¸»èŠ‚ç‚¹åœ°å€ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰')
    parser.add_argument('--master_port', type=str, default='12355',
                        help='ä¸»èŠ‚ç‚¹ç«¯å£ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰')
    
    args = parser.parse_args()
    
    # è·å–é…ç½®
    config = get_default_config()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.num_epochs:
        config['num_epochs'] = args.num_epochs
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.lr:
        config['learning_rate'] = args.lr
    if args.loss_type:
        config['loss_type'] = args.loss_type
    if args.classification_method:
        config['classification_method'] = args.classification_method
    if args.unified_threshold:
        config['unified_threshold'] = args.unified_threshold
    
    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    if args.use_distributed:
        config['use_distributed'] = True
    if args.gpus:
        config['gpus'] = args.gpus
    if args.master_addr:
        config['master_addr'] = args.master_addr
    if args.master_port:
        config['master_port'] = args.master_port
    
    # å¦‚æœæä¾›äº†é…ç½®æ–‡ä»¶ï¼ŒåŠ è½½å¹¶åˆå¹¶
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            user_config = json.load(f)
            config.update(user_config)
    
    # éªŒè¯GPUé…ç½®
    config, is_valid, error_msg = validate_gpu_config(config)
    if not is_valid:
        print(error_msg)
        return
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒè¿‡ç¨‹...")
    print(f"ğŸ“‹ æœ€ç»ˆé…ç½®:")
    print(f"  - åˆ†å¸ƒå¼è®­ç»ƒ: {config['use_distributed']}")
    print(f"  - ä½¿ç”¨GPU: {config['gpus']}")
    print(f"  - æ‰¹å¤§å°: {config['batch_size']}")
    print(f"  - å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"  - åˆ†ç±»æ–¹æ³•: {config['classification_method']}")
    
    # å¯åŠ¨è®­ç»ƒ
    if config['use_distributed']:
        # åˆ†å¸ƒå¼è®­ç»ƒ
        world_size = len(config['gpus'])
        try:
            mp.spawn(
                run_distributed_training,
                args=(world_size, config),
                nprocs=world_size,
                join=True
            )
            print("ğŸ‰ åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼")
        except Exception as e:
            print(f"âŒ åˆ†å¸ƒå¼è®­ç»ƒå¤±è´¥: {e}")
    else:
        # å•GPUè®­ç»ƒ
        try:
            # è®¾ç½®CUDAè®¾å¤‡
            if torch.cuda.is_available() and config['gpus']:
                torch.cuda.set_device(config['gpus'][0])
            
            # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
            trainer = BaseClassTrainer(config)
            
            # è®¾ç½®æ‰€æœ‰ç»„ä»¶
            trainer.setup_data_loaders()
            trainer.setup_model()
            trainer.setup_loss_function()
            trainer.setup_optimizer()
            trainer.setup_logging()
            
            # å¼€å§‹è®­ç»ƒ
            trainer.train()
            print("ğŸ‰ å•GPUè®­ç»ƒå®Œæˆï¼")
        except Exception as e:
            print(f"âŒ å•GPUè®­ç»ƒå¤±è´¥: {e}")


if __name__ == '__main__':
    main() 