"""
å¤šæ ‡ç­¾åˆ†ç±»è¯„ä¼°æŒ‡æ ‡æ¨¡å—
åŒ…å«é€‚åˆç±»åˆ«ä¸å‡è¡¡çš„è¯„ä¼°æŒ‡æ ‡
"""

import torch
import numpy as np
from sklearn.metrics import (
    average_precision_score, 
    precision_recall_curve,
    roc_auc_score,
    classification_report,
    multilabel_confusion_matrix,
    precision_score,
    recall_score,
    f1_score
)
import warnings
warnings.filterwarnings('ignore')


class MultiLabelMetrics:
    """å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def compute_metrics(logits, targets, threshold=0.5):
        """
        è®¡ç®—å¤šæ ‡ç­¾åˆ†ç±»çš„å„ç§æŒ‡æ ‡
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºlogits, shape=(batch, num_classes)
            targets: çœŸå®æ ‡ç­¾, shape=(batch, num_classes)
            threshold: åˆ†ç±»é˜ˆå€¼
            
        Returns:
            dict: åŒ…å«å„ç§æŒ‡æ ‡çš„å­—å…¸
        """
        # è½¬æ¢ä¸ºnumpy
        if torch.is_tensor(logits):
            logits = logits.detach().cpu().numpy()
        if torch.is_tensor(targets):
            targets = targets.detach().cpu().numpy()
        
        # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
        probs = 1 / (1 + np.exp(-logits))  # sigmoid
        predictions = (probs >= threshold).astype(int)
        
        metrics = {}
        
        # 1. Mean Average Precision (mAP)
        try:
            metrics['mAP'] = average_precision_score(targets, probs, average='macro')
            metrics['mAP_micro'] = average_precision_score(targets, probs, average='micro')
        except:
            metrics['mAP'] = 0.0
            metrics['mAP_micro'] = 0.0
        
        # 2. å„ç±»åˆ«çš„Average Precision
        try:
            ap_scores = average_precision_score(targets, probs, average=None)
            metrics['per_class_ap'] = ap_scores.tolist() if ap_scores is not None else []
        except:
            metrics['per_class_ap'] = []
        
        # 3. Precision, Recall, F1 (macroå’Œmicroå¹³å‡)
        try:
            metrics['precision_macro'] = precision_score(targets, predictions, average='macro', zero_division=0)
            metrics['recall_macro'] = recall_score(targets, predictions, average='macro', zero_division=0)
            metrics['f1_macro'] = f1_score(targets, predictions, average='macro', zero_division=0)
            
            metrics['precision_micro'] = precision_score(targets, predictions, average='micro', zero_division=0)
            metrics['recall_micro'] = recall_score(targets, predictions, average='micro', zero_division=0)
            metrics['f1_micro'] = f1_score(targets, predictions, average='micro', zero_division=0)
        except:
            metrics.update({
                'precision_macro': 0.0, 'recall_macro': 0.0, 'f1_macro': 0.0,
                'precision_micro': 0.0, 'recall_micro': 0.0, 'f1_micro': 0.0
            })
        
        # 4. å„ç±»åˆ«çš„Precision, Recall, F1
        try:
            per_class_precision = precision_score(targets, predictions, average=None, zero_division=0)
            per_class_recall = recall_score(targets, predictions, average=None, zero_division=0)
            per_class_f1 = f1_score(targets, predictions, average=None, zero_division=0)
            
            metrics['per_class_precision'] = per_class_precision.tolist()
            metrics['per_class_recall'] = per_class_recall.tolist()
            metrics['per_class_f1'] = per_class_f1.tolist()
        except:
            num_classes = targets.shape[1]
            metrics['per_class_precision'] = [0.0] * num_classes
            metrics['per_class_recall'] = [0.0] * num_classes
            metrics['per_class_f1'] = [0.0] * num_classes
        
        # 5. ROC AUCï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            metrics['roc_auc_macro'] = roc_auc_score(targets, probs, average='macro')
            metrics['roc_auc_micro'] = roc_auc_score(targets, probs, average='micro')
        except:
            metrics['roc_auc_macro'] = 0.0
            metrics['roc_auc_micro'] = 0.0
        
        # 6. ç®€åŒ–çš„èšåˆæŒ‡æ ‡ï¼ˆç”¨äºç›‘æ§ï¼‰
        metrics['avg_precision'] = metrics['precision_macro']
        metrics['avg_recall'] = metrics['recall_macro']
        metrics['avg_f1'] = metrics['f1_macro']
        
        # 7. æ ·æœ¬çº§æŒ‡æ ‡
        sample_precision = MultiLabelMetrics._compute_sample_precision(targets, predictions)
        sample_recall = MultiLabelMetrics._compute_sample_recall(targets, predictions)
        sample_f1 = MultiLabelMetrics._compute_sample_f1(targets, predictions)
        
        metrics['sample_precision'] = np.mean(sample_precision)
        metrics['sample_recall'] = np.mean(sample_recall)
        metrics['sample_f1'] = np.mean(sample_f1)
        
        # 8. ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
        metrics['positive_rate'] = np.mean(targets)
        metrics['prediction_rate'] = np.mean(predictions)
        metrics['num_classes'] = targets.shape[1]
        metrics['num_samples'] = targets.shape[0]
        
        return metrics
    
    @staticmethod
    def _compute_sample_precision(targets, predictions):
        """è®¡ç®—æ ·æœ¬çº§ç²¾ç¡®ç‡"""
        sample_precision = []
        for i in range(targets.shape[0]):
            tp = np.sum(targets[i] * predictions[i])
            fp = np.sum((1 - targets[i]) * predictions[i])
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            sample_precision.append(precision)
        return np.array(sample_precision)
    
    @staticmethod
    def _compute_sample_recall(targets, predictions):
        """è®¡ç®—æ ·æœ¬çº§å¬å›ç‡"""
        sample_recall = []
        for i in range(targets.shape[0]):
            tp = np.sum(targets[i] * predictions[i])
            fn = np.sum(targets[i] * (1 - predictions[i]))
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            sample_recall.append(recall)
        return np.array(sample_recall)
    
    @staticmethod
    def _compute_sample_f1(targets, predictions):
        """è®¡ç®—æ ·æœ¬çº§F1åˆ†æ•°"""
        sample_f1 = []
        for i in range(targets.shape[0]):
            tp = np.sum(targets[i] * predictions[i])
            fp = np.sum((1 - targets[i]) * predictions[i])
            fn = np.sum(targets[i] * (1 - predictions[i]))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            sample_f1.append(f1)
        return np.array(sample_f1)
    
    @staticmethod
    def find_optimal_thresholds(logits, targets, metric='f1'):
        """
        ä¸ºæ¯ä¸ªç±»åˆ«æ‰¾åˆ°æœ€ä¼˜åˆ†ç±»é˜ˆå€¼
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºlogits
            targets: çœŸå®æ ‡ç­¾
            metric: ä¼˜åŒ–æŒ‡æ ‡ ('f1', 'precision', 'recall')
            
        Returns:
            optimal_thresholds: æ¯ä¸ªç±»åˆ«çš„æœ€ä¼˜é˜ˆå€¼
        """
        if torch.is_tensor(logits):
            logits = logits.detach().cpu().numpy()
        if torch.is_tensor(targets):
            targets = targets.detach().cpu().numpy()
        
        probs = 1 / (1 + np.exp(-logits))
        num_classes = targets.shape[1]
        optimal_thresholds = []
        
        for class_idx in range(num_classes):
            class_targets = targets[:, class_idx]
            class_probs = probs[:, class_idx]
            
            if np.sum(class_targets) == 0:  # æ²¡æœ‰æ­£æ ·æœ¬
                optimal_thresholds.append(0.5)
                continue
            
            # è®¡ç®—precision-recallæ›²çº¿
            precision, recall, thresholds = precision_recall_curve(class_targets, class_probs)
            
            if metric == 'f1':
                f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
                best_idx = np.argmax(f1_scores)
            elif metric == 'precision':
                best_idx = np.argmax(precision)
            elif metric == 'recall':
                best_idx = np.argmax(recall)
            else:
                best_idx = len(thresholds) // 2  # é»˜è®¤é€‰æ‹©ä¸­ä½æ•°
            
            if best_idx < len(thresholds):
                optimal_thresholds.append(thresholds[best_idx])
            else:
                optimal_thresholds.append(0.5)
        
        return np.array(optimal_thresholds)
    
    @staticmethod
    def print_metrics_summary(metrics, top_k=10):
        """æ‰“å°æŒ‡æ ‡æ‘˜è¦"""
        print("ğŸ“Š å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡æ‘˜è¦:")
        print(f"  æ•°æ®ç»Ÿè®¡:")
        print(f"    - æ ·æœ¬æ•°: {metrics['num_samples']}")
        print(f"    - ç±»åˆ«æ•°: {metrics['num_classes']}")
        print(f"    - æ­£æ ·æœ¬æ¯”ä¾‹: {metrics['positive_rate']:.4f}")
        print(f"    - é¢„æµ‹é˜³æ€§æ¯”ä¾‹: {metrics['prediction_rate']:.4f}")
        
        print(f"\n  ä¸»è¦æŒ‡æ ‡:")
        print(f"    - mAP: {metrics['mAP']:.4f}")
        print(f"    - Precision (Macro): {metrics['precision_macro']:.4f}")
        print(f"    - Recall (Macro): {metrics['recall_macro']:.4f}")
        print(f"    - F1 (Macro): {metrics['f1_macro']:.4f}")
        
        print(f"\n  Microå¹³å‡:")
        print(f"    - Precision (Micro): {metrics['precision_micro']:.4f}")
        print(f"    - Recall (Micro): {metrics['recall_micro']:.4f}")
        print(f"    - F1 (Micro): {metrics['f1_micro']:.4f}")
        
        print(f"\n  æ ·æœ¬çº§æŒ‡æ ‡:")
        print(f"    - Sample Precision: {metrics['sample_precision']:.4f}")
        print(f"    - Sample Recall: {metrics['sample_recall']:.4f}")
        print(f"    - Sample F1: {metrics['sample_f1']:.4f}")
        
        # æ˜¾ç¤ºè¡¨ç°æœ€å¥½çš„ç±»åˆ«
        if 'per_class_f1' in metrics and len(metrics['per_class_f1']) > 0:
            per_class_f1 = np.array(metrics['per_class_f1'])
            best_classes = np.argsort(per_class_f1)[-top_k:][::-1]
            
            print(f"\n  è¡¨ç°æœ€ä½³çš„{min(top_k, len(best_classes))}ä¸ªç±»åˆ«:")
            for i, class_idx in enumerate(best_classes):
                f1 = per_class_f1[class_idx]
                precision = metrics['per_class_precision'][class_idx]
                recall = metrics['per_class_recall'][class_idx]
                print(f"    ç±»åˆ«{class_idx}: F1={f1:.4f}, P={precision:.4f}, R={recall:.4f}")


def test_metrics():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡"""
    print("æµ‹è¯•å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡...")
    
    # æ¨¡æ‹Ÿæ•°æ®
    batch_size = 100
    num_classes = 60
    
    # æ¨¡æ‹Ÿlogitså’Œtargets
    logits = torch.randn(batch_size, num_classes)
    targets = torch.zeros(batch_size, num_classes)
    
    # åˆ›å»ºä¸å‡è¡¡çš„æ­£æ ‡ç­¾ï¼ˆæ¯ä¸ªæ ·æœ¬2-4ä¸ªæ­£æ ‡ç­¾ï¼‰
    for i in range(batch_size):
        num_positive = np.random.randint(2, 5)
        pos_indices = torch.randperm(num_classes)[:num_positive]
        targets[i, pos_indices] = 1.0
    
    print(f"æ•°æ®å½¢çŠ¶: logits={logits.shape}, targets={targets.shape}")
    print(f"æ­£æ ·æœ¬æ¯”ä¾‹: {targets.mean():.4f}")
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = MultiLabelMetrics.compute_metrics(logits, targets)
    
    # æ‰“å°æ‘˜è¦
    MultiLabelMetrics.print_metrics_summary(metrics)
    
    # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
    print(f"\nğŸ” å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼...")
    optimal_thresholds = MultiLabelMetrics.find_optimal_thresholds(logits, targets, metric='f1')
    print(f"æœ€ä¼˜é˜ˆå€¼èŒƒå›´: [{optimal_thresholds.min():.4f}, {optimal_thresholds.max():.4f}]")
    print(f"å¹³å‡æœ€ä¼˜é˜ˆå€¼: {optimal_thresholds.mean():.4f}")


if __name__ == '__main__':
    test_metrics() 