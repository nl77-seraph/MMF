"""
å¤šæ ‡ç­¾åˆ†ç±»è¯„ä¼°æŒ‡æ ‡æ¨¡å—
åŒ…å«é€‚åˆç±»åˆ«ä¸å‡è¡¡çš„è¯„ä¼°æŒ‡æ ‡
"""

import torch
import numpy as np
import math
from sklearn.metrics import (
    average_precision_score, 
    precision_recall_curve,
    roc_auc_score,
    classification_report,
    multilabel_confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)
import warnings
warnings.filterwarnings('ignore')


def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    x = x - np.max(x, axis=-1, keepdims=True)
    x = np.exp(x)
    x = x / np.sum(x, axis=-1, keepdims=True)
    return x


class MultiLabelMetrics:
    """å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def compute_metrics(logits, targets,config):
        """
        è®¡ç®—å¤šæ ‡ç­¾åˆ†ç±»çš„å„ç§æŒ‡æ ‡
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºlogits, shape=(batch, num_classes)
            targets: çœŸå®æ ‡ç­¾, shape=(batch, num_classes)
            threshold: åˆ†ç±»é˜ˆå€¼
            
        Returns:
            dict: åŒ…å«å„ç§æŒ‡æ ‡çš„å­—å…¸
        """
        # è½¬æ¢ä¸ºnumpy
        if torch.is_tensor(logits):
            logits = logits.detach().cpu().numpy()
        if torch.is_tensor(targets):
            targets = targets.detach().cpu().numpy()
        
        # è®¡ç®—æ¦‚ç‡å’Œé¢„æµ‹
        sig_probs = sigmoid(logits)
        soft_probs = softmax(logits)
        #predictions = (probs >= threshold).astype(int)
        
        metrics = {}
        try:
            metrics['soft_mAP'] = average_precision_score(targets, soft_probs, average='macro')
            metrics['sig_mAP'] = average_precision_score(targets, sig_probs, average='macro')
        except:
            metrics['soft_mAP'] = 0.0
            metrics['sig_mAP'] = 0.0
        try:
            metrics['soft_roc_auc'] = roc_auc_score(targets, soft_probs, average='macro')
            metrics['sig_roc_auc'] = roc_auc_score(targets, sig_probs, average='macro')
        except:
            metrics['soft_roc_auc'] = 0.0
            metrics['sig_roc_auc'] = 0.0
        if config['tabs'] == 'mixed':
            metrics['pk'] = MultiLabelMetrics.precision_at_dynamic_k(targets, soft_probs)
            metrics['mapk'] = 0.0
        else:
            metrics['pk'] = MultiLabelMetrics.precision_at_k(targets, soft_probs, int(config['tabs']))
            metrics['mapk'] = MultiLabelMetrics.average_precision_at_k(targets, soft_probs, int(config['tabs']))

        
        return metrics
    
    @staticmethod
    def precision_at_k(y_true, y_pred, k):
        top_k_preds = np.argsort(y_pred, axis=1)[:, -k:]
        precisions = []

        for i in range(y_true.shape[0]):
            true_positives = np.sum(y_true[i, top_k_preds[i]])
            precisions.append(true_positives / k)

        return np.mean(precisions)
    @staticmethod
    def precision_at_dynamic_k(y_true, y_pred):
        precisions = []
        for i in range(y_true.shape[0]):
            k = int(np.sum(y_true[i]))
            if k == 0:
                precisions.append(1)
                continue
            top_k_preds = np.argsort(y_pred[i])[-k:] 
            true_positives = np.sum(y_true[i, top_k_preds])
            precisions.append(true_positives / k)
        return np.mean(precisions) if precisions else 0.0
    @staticmethod
    def average_precision_at_k(y_true, y_pred, k):
        res = 0
        for i in range(k):
            res += MultiLabelMetrics.precision_at_k(y_true, y_pred, i+1)
        res /= k
        return res

    @staticmethod
    def print_metrics_summary(metrics):
        """æ‰“å°æŒ‡æ ‡æ‘˜è¦"""
        print("ğŸ“Š å¤šæ ‡ç­¾åˆ†ç±»æŒ‡æ ‡æ‘˜è¦:")
        print("Metrics: | soft_mAP | sig_mAP | soft_roc_auc | sig_roc_auc | pk | mapk |")
        print(f"Values:  | {metrics.get('soft_mAP', 0.0):.4f}   | {metrics.get('sig_mAP', 0.0):.4f}  | {metrics.get('soft_roc_auc', 0.0):.4f}       | {metrics.get('sig_roc_auc', 0.0):.4f}      | {metrics.get('pk', 0.0):.4f} | {metrics.get('mapk', 0.0):.4f} |")

    @staticmethod
    def _compute_sample_precision(targets, predictions):
        """è®¡ç®—æ ·æœ¬çº§ç²¾ç¡®ç‡"""
        sample_precision = []
        for i in range(targets.shape[0]):
            tp = np.sum(targets[i] * predictions[i])
            fp = np.sum((1 - targets[i]) * predictions[i])
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            sample_precision.append(precision)
        return np.array(sample_precision)
    
    @staticmethod
    def _compute_sample_recall(targets, predictions):
        """è®¡ç®—æ ·æœ¬çº§å¬å›ç‡"""
        sample_recall = []
        for i in range(targets.shape[0]):
            tp = np.sum(targets[i] * predictions[i])
            fn = np.sum(targets[i] * (1 - predictions[i]))
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            sample_recall.append(recall)
        return np.array(sample_recall)
    
    @staticmethod
    def _compute_sample_f1(targets, predictions):
        """è®¡ç®—æ ·æœ¬çº§F1åˆ†æ•°"""
        sample_f1 = []
        for i in range(targets.shape[0]):
            tp = np.sum(targets[i] * predictions[i])
            fp = np.sum((1 - targets[i]) * predictions[i])
            fn = np.sum(targets[i] * (1 - predictions[i]))
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            sample_f1.append(f1)
        return np.array(sample_f1)
    
    @staticmethod
    def find_optimal_thresholds(logits, targets, metric='f1'):
        """
        ä¸ºæ¯ä¸ªç±»åˆ«æ‰¾åˆ°æœ€ä¼˜åˆ†ç±»é˜ˆå€¼
        
        Args:
            logits: æ¨¡å‹è¾“å‡ºlogits
            targets: çœŸå®æ ‡ç­¾
            metric: ä¼˜åŒ–æŒ‡æ ‡ ('f1', 'precision', 'recall')
            
        Returns:
            optimal_thresholds: æ¯ä¸ªç±»åˆ«çš„æœ€ä¼˜é˜ˆå€¼
        """
        if torch.is_tensor(logits):
            logits = logits.detach().cpu().numpy()
        if torch.is_tensor(targets):
            targets = targets.detach().cpu().numpy()
        
        probs = 1 / (1 + np.exp(-logits))
        num_classes = targets.shape[1]
        optimal_thresholds = []
        
        for class_idx in range(num_classes):
            class_targets = targets[:, class_idx]
            class_probs = probs[:, class_idx]
            
            if np.sum(class_targets) == 0:  # æ²¡æœ‰æ­£æ ·æœ¬
                optimal_thresholds.append(0.5)
                continue
            
            # è®¡ç®—precision-recallæ›²çº¿
            precision, recall, thresholds = precision_recall_curve(class_targets, class_probs)
            
            if metric == 'f1':
                f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
                best_idx = np.argmax(f1_scores)
            elif metric == 'precision':
                best_idx = np.argmax(precision)
            elif metric == 'recall':
                best_idx = np.argmax(recall)
            else:
                best_idx = len(thresholds) // 2  # é»˜è®¤é€‰æ‹©ä¸­ä½æ•°
            
            if best_idx < len(thresholds):
                optimal_thresholds.append(thresholds[best_idx])
            else:
                optimal_thresholds.append(0.5)
        
        return np.array(optimal_thresholds)
    
   
        
