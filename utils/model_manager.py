"""
æ¨¡å‹ç®¡ç†å™¨
è´Ÿè´£æ¨¡å‹ä¿å­˜ã€åŠ è½½å’Œcheckpointç®¡ç†
"""

import torch
import os
import json
import shutil
from datetime import datetime
from typing import Dict, Any, Optional
import glob


class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_dir: str):
        """
        Args:
            checkpoint_dir: checkpointä¿å­˜ç›®å½•
        """
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„
        self.best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')
        self.latest_model_path = os.path.join(checkpoint_dir, 'latest_model.pth')
        self.metrics_history_path = os.path.join(checkpoint_dir, 'metrics_history.json')
        
        # æŒ‡æ ‡å†å²
        self.metrics_history = []
        self.load_metrics_history()
        
        print(f"ğŸ“ æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–: {checkpoint_dir}")
    
    def save_checkpoint(self, model, optimizer, scheduler, epoch, metrics, is_best=False):
        """
        ä¿å­˜checkpoint
        
        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
            epoch: å½“å‰epoch
            metrics: è¯„ä¼°æŒ‡æ ‡
            is_best: æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
        """
        # å‡†å¤‡ä¿å­˜çš„çŠ¶æ€
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜æœ€æ–°æ¨¡å‹
        #torch.save(checkpoint, self.latest_model_path)
        
        # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œä¿å­˜åˆ°best_model
        if is_best:
            torch.save(checkpoint, self.best_model_path)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: epoch {epoch+1}, mAP={metrics.get('sig_mAP', 0):.4f}")
            # æ›´æ–°æŒ‡æ ‡å†å²
            self.metrics_history.append({
                'epoch': epoch,
                'metrics': metrics
            })
            self.save_metrics_history()
        # # ä¿å­˜å®šæœŸcheckpoint
        # if (epoch + 1) % 50 == 0:
        #     epoch_checkpoint_path = os.path.join(
        #         self.checkpoint_dir, 
        #         f'checkpoint_epoch_{epoch+1}.pth'
        #     )
        #     torch.save(checkpoint, epoch_checkpoint_path)
        

    
    def load_checkpoint(self, model, optimizer=None, scheduler=None, 
                       checkpoint_path=None, load_best=True):
        """
        åŠ è½½checkpoint
        
        Args:
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
            scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼‰
            checkpoint_path: æŒ‡å®šçš„checkpointè·¯å¾„
            load_best: æ˜¯å¦åŠ è½½æœ€ä½³æ¨¡å‹
            
        Returns:
            loaded_info: åŠ è½½ä¿¡æ¯å­—å…¸
        """
        # ç¡®å®šè¦åŠ è½½çš„checkpointè·¯å¾„
        if checkpoint_path is None:
            if load_best and os.path.exists(self.best_model_path):
                checkpoint_path = self.best_model_path
            elif os.path.exists(self.latest_model_path):
                checkpoint_path = self.latest_model_path
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„checkpoint")
                return None
        
        if not os.path.exists(checkpoint_path):
            print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return None
        
        # åŠ è½½checkpoint
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # åŠ è½½æ¨¡å‹çŠ¶æ€
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if optimizer is not None and 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # åŠ è½½è°ƒåº¦å™¨çŠ¶æ€
            if (scheduler is not None and 
                'scheduler_state_dict' in checkpoint and 
                checkpoint['scheduler_state_dict'] is not None):
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            loaded_info = {
                'epoch': checkpoint.get('epoch', 0),
                'metrics': checkpoint.get('metrics', {}),
                'timestamp': checkpoint.get('timestamp', ''),
                'checkpoint_path': checkpoint_path
            }
            
            print(f"âœ… æˆåŠŸåŠ è½½checkpoint: {checkpoint_path}")
            print(f"   - Epoch: {loaded_info['epoch']}")
            print(f"   - mAP: {loaded_info['metrics'].get('mAP', 0):.4f}")
            
            return loaded_info
            
        except Exception as e:
            print(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            return None
    
    def load_model_only(self, model, checkpoint_path=None, load_best=True):
        """
        åªåŠ è½½æ¨¡å‹æƒé‡ï¼Œä¸åŠ è½½ä¼˜åŒ–å™¨ç­‰
        
        Args:
            model: æ¨¡å‹
            checkpoint_path: æŒ‡å®šçš„checkpointè·¯å¾„
            load_best: æ˜¯å¦åŠ è½½æœ€ä½³æ¨¡å‹
        """
        # ç¡®å®šè¦åŠ è½½çš„checkpointè·¯å¾„
        if checkpoint_path is None:
            if load_best and os.path.exists(self.best_model_path):
                checkpoint_path = self.best_model_path
            elif os.path.exists(self.latest_model_path):
                checkpoint_path = self.latest_model_path
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„checkpoint")
                return None
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            model.load_state_dict(checkpoint['model_state_dict'])
            
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
            return {
                'epoch': checkpoint.get('epoch', 0),
                'metrics': checkpoint.get('metrics', {})
            }
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æƒé‡å¤±è´¥: {e}")
            return None
    
    def save_metrics_history(self):
        """ä¿å­˜æŒ‡æ ‡å†å²"""
        try:
            with open(self.metrics_history_path, 'w') as f:
                json.dump(self.metrics_history, f, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æŒ‡æ ‡å†å²å¤±è´¥: {e}")
    
    def load_metrics_history(self):
        """åŠ è½½æŒ‡æ ‡å†å²"""
        if os.path.exists(self.metrics_history_path):
            try:
                with open(self.metrics_history_path, 'r') as f:
                    self.metrics_history = json.load(f)
                print(f"ğŸ“Š åŠ è½½æŒ‡æ ‡å†å²: {len(self.metrics_history)}æ¡è®°å½•")
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æŒ‡æ ‡å†å²å¤±è´¥: {e}")
                self.metrics_history = []
    
    def get_best_metrics(self):
        """è·å–æœ€ä½³æŒ‡æ ‡"""
        if not self.metrics_history:
            return None
        
        best_entry = max(self.metrics_history, 
                        key=lambda x: x['metrics'].get('mAP', 0))
        return best_entry
    
    def get_training_summary(self):
        """è·å–è®­ç»ƒæ‘˜è¦"""
        if not self.metrics_history:
            return {}
        
        # æå–æ‰€æœ‰mAPå€¼
        map_values = [entry['metrics'].get('mAP', 0) for entry in self.metrics_history]
        
        summary = {
            'total_epochs': len(self.metrics_history),
            'best_mAP': max(map_values) if map_values else 0,
            'final_mAP': map_values[-1] if map_values else 0,
            'mAP_improvement': map_values[-1] - map_values[0] if len(map_values) > 1 else 0,
            'best_epoch': max(self.metrics_history, 
                            key=lambda x: x['metrics'].get('mAP', 0))['epoch'] if self.metrics_history else 0
        }
        
        return summary
    
    def clean_old_checkpoints(self, keep_latest=5):
        """æ¸…ç†æ—§çš„epoch checkpointsï¼Œä¿ç•™æœ€æ–°çš„å‡ ä¸ª"""
        pattern = os.path.join(self.checkpoint_dir, 'checkpoint_epoch_*.pth')
        checkpoint_files = glob.glob(pattern)
        
        if len(checkpoint_files) <= keep_latest:
            return
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        checkpoint_files.sort(key=os.path.getmtime, reverse=True)
        
        # åˆ é™¤æ—§æ–‡ä»¶
        for old_file in checkpoint_files[keep_latest:]:
            try:
                os.remove(old_file)
                print(f"ğŸ—‘ï¸ åˆ é™¤æ—§checkpoint: {os.path.basename(old_file)}")
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤å¤±è´¥ {old_file}: {e}")
    
    def export_model(self, model, export_path=None, include_config=True):
        """
        å¯¼å‡ºæ¨¡å‹ç”¨äºéƒ¨ç½²
        
        Args:
            model: æ¨¡å‹
            export_path: å¯¼å‡ºè·¯å¾„
            include_config: æ˜¯å¦åŒ…å«é…ç½®ä¿¡æ¯
        """
        if export_path is None:
            export_path = os.path.join(self.checkpoint_dir, 'exported_model.pth')
        
        # ç¡®ä¿æ¨¡å‹åœ¨evalæ¨¡å¼
        model.eval()
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = {
            'model_state_dict': model.state_dict(),
            'export_timestamp': datetime.now().isoformat(),
        }
        
        # å¦‚æœæœ‰æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡ï¼Œæ·»åŠ è¿›å»
        best_metrics = self.get_best_metrics()
        if best_metrics:
            export_data['best_metrics'] = best_metrics['metrics']
            export_data['best_epoch'] = best_metrics['epoch']
        
        # ä¿å­˜
        torch.save(export_data, export_path)
        print(f"ğŸ“¦ æ¨¡å‹å·²å¯¼å‡º: {export_path}")
        
        return export_path
    
    def print_training_summary(self):
        """æ‰“å°è®­ç»ƒæ‘˜è¦"""
        summary = self.get_training_summary()
        
        if not summary:
            print("ğŸ“Š æš‚æ— è®­ç»ƒè®°å½•")
            return
        
        print("ğŸ“Š è®­ç»ƒæ‘˜è¦:")
        print(f"  - è®­ç»ƒè½®æ•°: {summary['total_epochs']}")
        print(f"  - æœ€ä½³mAP: {summary['best_mAP']:.4f} (Epoch {summary['best_epoch']+1})")
        print(f"  - æœ€ç»ˆmAP: {summary['final_mAP']:.4f}")
        print(f"  - mAPæå‡: {summary['mAP_improvement']:+.4f}")
        
        # æ˜¾ç¤ºå¯ç”¨çš„checkpoint
        available_checkpoints = []
        if os.path.exists(self.best_model_path):
            available_checkpoints.append("best_model.pth")
        if os.path.exists(self.latest_model_path):
            available_checkpoints.append("latest_model.pth")
        
        epoch_checkpoints = glob.glob(
            os.path.join(self.checkpoint_dir, 'checkpoint_epoch_*.pth')
        )
        available_checkpoints.extend([os.path.basename(f) for f in epoch_checkpoints])
        
        print(f"  - å¯ç”¨checkpoint: {len(available_checkpoints)}ä¸ª")
        for checkpoint in available_checkpoints[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
            print(f"    â€¢ {checkpoint}")
        if len(available_checkpoints) > 5:
            print(f"    â€¢ ... å’Œå…¶ä»–{len(available_checkpoints)-5}ä¸ª")


def test_model_manager():
    """æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨"""
    print("æµ‹è¯•æ¨¡å‹ç®¡ç†å™¨...")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    test_dir = "./test_checkpoints"
    manager = ModelManager(test_dir)
    
    # æ¨¡æ‹Ÿæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = torch.nn.Linear(10, 1)
    optimizer = torch.optim.Adam(model.parameters())
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)
    
    # æ¨¡æ‹Ÿè®­ç»ƒå‡ ä¸ªepoch
    for epoch in range(5):
        # æ¨¡æ‹ŸæŒ‡æ ‡
        metrics = {
            'mAP': 0.5 + epoch * 0.1,
            'precision_macro': 0.4 + epoch * 0.1,
            'recall_macro': 0.3 + epoch * 0.1
        }
        
        is_best = epoch == 3  # å‡è®¾ç¬¬4ä¸ªepochæ˜¯æœ€ä½³
        
        # ä¿å­˜checkpoint
        manager.save_checkpoint(
            model, optimizer, scheduler, epoch, metrics, is_best
        )
    
    # æ‰“å°è®­ç»ƒæ‘˜è¦
    manager.print_training_summary()
    
    # æµ‹è¯•åŠ è½½
    loaded_info = manager.load_checkpoint(model, optimizer, scheduler)
    print(f"\nåŠ è½½æµ‹è¯•: {loaded_info}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    shutil.rmtree(test_dir)
    print("\nâœ… æµ‹è¯•å®Œæˆï¼Œæ¸…ç†æµ‹è¯•æ–‡ä»¶")


if __name__ == '__main__':
    test_model_manager() 