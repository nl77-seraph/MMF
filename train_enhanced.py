"""
å¢å¼ºç‰ˆè®­ç»ƒè„šæœ¬
ä½¿ç”¨EnhancedMultiMetaFingerNetè¿›è¡ŒBase Classè®­ç»ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import os
import sys
import time
import json
from datetime import datetime
import argparse
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from data.meta_traffic_dataloader import MetaTrafficDataLoader
from models.feature_extractors_enhanced import EnhancedMultiMetaFingerNet  # ä½¿ç”¨å¢å¼ºç‰ˆ
from utils.metrics import MultiLabelMetrics
from utils.loss_functions import WeightedBCELoss, FocalLoss
from utils.model_manager import ModelManager


# å¤ç”¨åŸtrain.pyçš„è¾…åŠ©å‡½æ•°
from train import (
    setup_distributed_training,
    cleanup_distributed_training,
    is_main_process,
    validate_gpu_config
)


class EnhancedTrainer:
    """
    å¢å¼ºç‰ˆè®­ç»ƒå™¨
    ä¸BaseClassTraineråŸºæœ¬ç›¸åŒï¼Œä½†ä½¿ç”¨EnhancedMultiMetaFingerNet
    """
    
    def __init__(self, config, rank=None, world_size=None):
        self.config = config
        self.rank = rank if rank is not None else 0
        self.world_size = world_size if world_size is not None else 1
        self.is_distributed = world_size is not None and world_size > 1
        
        # è®¾ç½®è®¾å¤‡
        if self.is_distributed:
            self.device = torch.device(f'cuda:{rank}')
        else:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
        self.model = None
        self.train_loader = None
        self.val_loader = None
        self.criterion = None
        self.optimizer = None
        self.scheduler = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_map = 0.0
        self.train_losses = []
        self.val_metrics = []
        
        # æ—¥å¿—å’Œå¯è§†åŒ–ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰
        self.writer = None
        self.model_manager = None
        
        if is_main_process():
            print(f"ğŸš€ EnhancedTraineråˆå§‹åŒ– (æ··åˆæ–¹æ¡ˆC)")
            print(f"  - æ¨¡å¼: {'åˆ†å¸ƒå¼è®­ç»ƒ' if self.is_distributed else 'å•GPUè®­ç»ƒ'}")
            if self.is_distributed:
                print(f"  - Rank: {self.rank}/{self.world_size}")
            print(f"  - è®¾å¤‡: {self.device}")
            print(f"  - æ”¹è¿›: SE + Shot Attention + Cross-Class Attention")
    
    def setup_data_loaders(self):
        """è®¾ç½®æ•°æ®åŠ è½½å™¨ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
        if is_main_process():
            print("\nğŸ“¦ è®¾ç½®æ•°æ®åŠ è½½å™¨...")
        
        # è®­ç»ƒæ•°æ®åŠ è½½å™¨
        train_loader_base = MetaTrafficDataLoader(
            query_json_path=self.config['train_query_json'],
            query_files_dir=self.config['train_query_dir'],
            support_root_dir=self.config['support_root_dir'],
            activated_classes=list(range(self.config['num_classes'])),
            target_length=self.config['sequence_length'],
            shots_per_class=self.config['shots_per_class'],
            batch_size=self.config['batch_size'],
            shuffle=not self.is_distributed,
            num_workers=self.config['num_workers'],
            random_sampling=True
        )
        
        # åˆ†å¸ƒå¼é‡‡æ ·å™¨
        if self.is_distributed:
            from torch.utils.data import DataLoader
            self.train_sampler = DistributedSampler(
                train_loader_base.query_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True
            )
            train_loader_base.query_loader = DataLoader(
                train_loader_base.query_dataset,
                batch_size=self.config['batch_size'],
                sampler=self.train_sampler,
                num_workers=self.config['num_workers'],
                collate_fn=train_loader_base._query_collate_fn
            )
        else:
            self.train_sampler = None
        
        self.train_loader = train_loader_base
        
        # éªŒè¯æ•°æ®åŠ è½½å™¨
        self.val_loader = MetaTrafficDataLoader(
            query_json_path=self.config['val_query_json'],
            query_files_dir=self.config['val_query_dir'],
            support_root_dir=self.config['support_root_dir'],
            activated_classes=list(range(self.config['num_classes'])),
            target_length=self.config['sequence_length'],
            shots_per_class=self.config['shots_per_class'],
            batch_size=self.config['val_batch_size'],
            shuffle=False,
            num_workers=self.config['num_workers'],
            random_sampling=True
        )
        
        if is_main_process():
            print(f"  âœ… è®­ç»ƒé›†: {len(self.train_loader)} batches")
            print(f"  âœ… éªŒè¯é›†: {len(self.val_loader)} batches")
    
    def setup_model(self):
        """è®¾ç½®å¢å¼ºç‰ˆæ¨¡å‹"""
        if is_main_process():
            print("\nğŸ§  è®¾ç½®å¢å¼ºç‰ˆç½‘ç»œæ¨¡å‹...")
        
        # ä½¿ç”¨EnhancedMultiMetaFingerNet
        self.model = EnhancedMultiMetaFingerNet(
            num_classes=self.config['num_classes'],
            dropout=self.config['dropout'],
            support_blocks=self.config['support_blocks'],
            classification_method=self.config['classification_method'],
            unified_threshold=self.config['unified_threshold'],
            use_se_in_df=self.config.get('use_se_in_df', False)  # å¯é€‰çš„DFå¢å¼º
        ).to(self.device)
        
        # DDPåŒ…è£…
        if self.is_distributed:
            self.model = DDP(
                self.model,
                device_ids=[self.rank],
                output_device=self.rank,
                find_unused_parameters=False
            )
            if is_main_process():
                print(f"  âœ… DDPæ¨¡å‹åŒ…è£…å®Œæˆ")
        
        # è®¡ç®—å‚æ•°é‡
        if is_main_process():
            model_for_count = self.model.module if self.is_distributed else self.model
            total_params = sum(p.numel() for p in model_for_count.parameters())
            trainable_params = sum(p.numel() for p in model_for_count.parameters() if p.requires_grad)
            
            print(f"  âœ… æ¨¡å‹å‚æ•°: {total_params:,} æ€»é‡, {trainable_params:,} å¯è®­ç»ƒ")
    
    def setup_loss_function(self):
        """è®¾ç½®æŸå¤±å‡½æ•°ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
        if is_main_process():
            print("\nâš–ï¸ è®¾ç½®æŸå¤±å‡½æ•°...")
        
        positive_ratio = self.config['positive_ratio']
        pos_weight = torch.tensor([positive_ratio] * self.config['num_classes']).to(self.device)
        self.pos_weight = pos_weight
        
        loss_type = self.config['loss_type']
        
        if loss_type == 'weighted_bce':
            self.criterion = WeightedBCELoss(pos_weight=pos_weight)
        elif loss_type == 'focal':
            self.criterion = FocalLoss(
                alpha=self.config['focal_alpha'],
                gamma=self.config['focal_gamma'],
                pos_weight=pos_weight
            )
        else:
            self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        
        if is_main_process():
            print(f"  âœ… æŸå¤±å‡½æ•°: {loss_type}")
    
    def setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
        if is_main_process():
            print("\nğŸ¯ è®¾ç½®ä¼˜åŒ–å™¨...")
        
        model_params = self.model.module.parameters() if self.is_distributed else self.model.parameters()
        
        if self.config['optimizer'] == 'adam':
            self.optimizer = optim.Adam(
                model_params,
                lr=self.config['learning_rate'],
                weight_decay=self.config['weight_decay']
            )
        elif self.config['optimizer'] == 'sgd':
            self.optimizer = optim.SGD(
                model_params,
                lr=self.config['learning_rate'],
                momentum=self.config['momentum'],
                weight_decay=self.config['weight_decay']
            )
        
        if self.config['scheduler'] == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config['num_epochs'],
                eta_min=self.config['min_lr']
            )
        elif self.config['scheduler'] == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config['step_size'],
                gamma=self.config['gamma']
            )
        
        if is_main_process():
            print(f"  âœ… ä¼˜åŒ–å™¨: {self.config['optimizer']}")
            print(f"  âœ… å­¦ä¹ ç‡: {self.config['learning_rate']}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ï¼ˆä»…ä¸»è¿›ç¨‹ï¼‰"""
        if not is_main_process():
            return
            
        print("\nğŸ“Š è®¾ç½®æ—¥å¿—ç³»ç»Ÿ...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        mode_suffix = "_ddp" if self.is_distributed else "_single"
        exp_name = f"enhanced_training_{timestamp}{mode_suffix}"
        self.exp_dir = os.path.join(self.config['output_dir'], exp_name)
        os.makedirs(self.exp_dir, exist_ok=True)
        
        log_dir = os.path.join(self.exp_dir, 'logs')
        self.writer = SummaryWriter(log_dir)
        
        checkpoint_dir = os.path.join(self.exp_dir, 'checkpoints')
        self.model_manager = ModelManager(checkpoint_dir)
        
        print(f"  âœ… å®éªŒç›®å½•: {self.exp_dir}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆä¸åŸç‰ˆé€»è¾‘ç›¸åŒï¼‰"""
        self.model.train()
        
        if self.is_distributed and self.train_sampler:
            self.train_sampler.set_epoch(epoch)
        
        train_losses = []
        all_train_logits = []
        all_train_labels = []
        
        for batch_idx, batch in enumerate(self.train_loader):
            query_data, support_data, support_masks, batch_info = batch
            
            query_data = query_data.to(self.device)
            support_data = support_data.to(self.device)
            support_masks = support_masks.to(self.device)
            query_labels = batch_info['query_labels'].to(self.device)
            
            results = self.model(query_data, support_data, support_masks)
            loss = self.criterion(results['logits'], query_labels.float())
            
            self.optimizer.zero_grad()
            loss.backward()
            
            if self.config.get('grad_clip', 0) > 0:
                model_params = self.model.module.parameters() if self.is_distributed else self.model.parameters()
                torch.nn.utils.clip_grad_norm_(model_params, self.config['grad_clip'])
            
            self.optimizer.step()
            
            train_losses.append(loss.item())
            all_train_logits.append(results['logits'].detach().cpu())
            all_train_labels.append(query_labels.detach().cpu())
            
            if is_main_process() and batch_idx % 10 == 0:
                step = epoch * len(self.train_loader) + batch_idx
                self.writer.add_scalar('Train/BatchLoss', loss.item(), step)
                self.writer.add_scalar('Train/LearningRate', self.optimizer.param_groups[0]['lr'], step)
        
        avg_train_loss = np.mean(train_losses)
        all_train_logits = torch.cat(all_train_logits, dim=0)
        all_train_labels = torch.cat(all_train_labels, dim=0)
        train_metrics = MultiLabelMetrics.compute_metrics(all_train_logits, all_train_labels)
        
        if is_main_process():
            self.writer.add_scalar('Train/EpochLoss', avg_train_loss, epoch)
            self.writer.add_scalar('Train/mAP', train_metrics['mAP'], epoch)
        
        return avg_train_loss, train_metrics
    
    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepochï¼ˆä¸åŸç‰ˆç›¸åŒï¼‰"""
        self.model.eval()
        val_losses = []
        all_logits = []
        all_labels = []
        
        with torch.no_grad():
            for batch in self.val_loader:
                query_data, support_data, support_masks, batch_info = batch
                
                query_data = query_data.to(self.device)
                support_data = support_data.to(self.device)
                support_masks = support_masks.to(self.device)
                query_labels = batch_info['query_labels'].to(self.device)
                
                results = self.model(query_data, support_data, support_masks)
                loss = self.criterion(results['logits'], query_labels.float())
                val_losses.append(loss.item())
                
                all_logits.append(results['logits'].cpu())
                all_labels.append(query_labels.cpu())
        
        all_logits = torch.cat(all_logits, dim=0)
        all_labels = torch.cat(all_labels, dim=0)
        
        metrics = MultiLabelMetrics.compute_metrics(all_logits, all_labels)
        avg_val_loss = np.mean(val_losses)
        
        if is_main_process():
            self.writer.add_scalar('Val/EpochLoss', avg_val_loss, epoch)
            self.writer.add_scalar('Val/mAP', metrics['mAP'], epoch)
        
        return avg_val_loss, metrics
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        if is_main_process():
            print("\nğŸš€ å¼€å§‹å¢å¼ºç‰ˆBase Classè®­ç»ƒ...")
            print(f"  - è®­ç»ƒè½®æ•°: {self.config['num_epochs']}")
            print(f"  - ç›®æ ‡: ä»0.9+ mAPæå‡åˆ°0.95+ mAP")
        
        for epoch in range(self.config['num_epochs']):
            self.current_epoch = epoch
            
            train_loss, train_metrics = self.train_epoch(epoch)
            self.train_losses.append(train_loss)
            
            val_loss, val_metrics = self.validate_epoch(epoch)
            self.val_metrics.append(val_metrics)
            
            if is_main_process():
                print(f"\nEpoch {epoch+1}/{self.config['num_epochs']}:")
                print(f"  ğŸ“ˆ Train - Loss:{train_loss:.4f}, mAP:{train_metrics['mAP']:.4f}")
                print(f"  ğŸ“Š Val   - Loss:{val_loss:.4f}, mAP:{val_metrics['mAP']:.4f}")
                
                is_best = val_metrics['mAP'] > self.best_map
                if is_best:
                    self.best_map = val_metrics['mAP']
                    print(f"  ğŸ‰ æ–°æœ€ä½³mAP: {self.best_map:.4f}")
                
                model_to_save = self.model.module if self.is_distributed else self.model
                self.model_manager.save_checkpoint(
                    model=model_to_save,
                    optimizer=self.optimizer,
                    scheduler=self.scheduler,
                    epoch=epoch,
                    metrics=val_metrics,
                    is_best=is_best
                )
            
            if self.is_distributed:
                dist.barrier()
            
            if self.scheduler:
                self.scheduler.step()
        
        if is_main_process():
            print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³mAP: {self.best_map:.4f}")
            if self.writer:
                self.writer.close()


def run_distributed_training(rank, world_size, config):
    """åˆ†å¸ƒå¼è®­ç»ƒå·¥ä½œå‡½æ•°"""
    try:
        setup_distributed_training(rank, world_size, config)
        
        trainer = EnhancedTrainer(config, rank=rank, world_size=world_size)
        trainer.setup_data_loaders()
        trainer.setup_model()
        trainer.setup_loss_function()
        trainer.setup_optimizer()
        trainer.setup_logging()
        trainer.train()
        
    except Exception as e:
        print(f"âŒ Rank {rank} è®­ç»ƒå¤±è´¥: {e}")
        raise e
    finally:
        cleanup_distributed_training()


def get_default_config():
    """è·å–é»˜è®¤é…ç½®ï¼ˆä¸åŸç‰ˆç›¸åŒï¼Œæ·»åŠ å¢å¼ºé€‰é¡¹ï¼‰"""
    return {
        # æ•°æ®é…ç½®
        'train_query_json': 'datasets/3tab_exp/base_train/3tab_train.json',
        'train_query_dir': 'datasets/3tab_exp/base_train/query_data',
        'val_query_json': 'datasets/3tab_exp/base_train/3tab_val.json',
        'val_query_dir': 'datasets/3tab_exp/base_train/query_data',
        'support_root_dir': 'datasets/3tab_exp/base_train/support_data',
        
        # æ¨¡å‹é…ç½®
        'num_classes': 60,
        'sequence_length': 30000,
        'shots_per_class': 1,
        'support_blocks': 0,
        'dropout': 0.15,
        
        # å¢å¼ºé€‰é¡¹
        'use_se_in_df': False,  # æ˜¯å¦åœ¨DFä¸­ä½¿ç”¨SE Block
        
        # åˆ†ç±»å¤´é…ç½®
        'classification_method': 'binary',
        'unified_threshold': 0.4,
        
        # è®­ç»ƒé…ç½®
        'num_epochs': 100,
        'batch_size': 8,
        'val_batch_size': 8,
        'num_workers': 0,
        
        # ä¼˜åŒ–å™¨é…ç½®
        'optimizer': 'adam',
        'learning_rate': 5e-5,
        'weight_decay': 1e-4,
        'momentum': 0.9,
        'grad_clip': 1.0,
        
        # å­¦ä¹ ç‡è°ƒåº¦
        'scheduler': 'cosine',
        'step_size': 30,
        'gamma': 0.1,
        'min_lr': 1e-6,
        
        # æŸå¤±å‡½æ•°é…ç½®
        'loss_type': 'weighted_bce',
        'positive_ratio': 19.0,
        'focal_alpha': 0.25,
        'focal_gamma': 2.0,
        
        # åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
        'use_distributed': False,
        'gpus': [0],
        'dist_backend': 'nccl',
        'master_addr': 'localhost',
        'master_port': '12355',
        
        # è¾“å‡ºé…ç½®
        'output_dir': './experiments',
        'save_interval': 10,
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Enhanced Training with Mixed Scheme C')
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--num_epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=8, help='æ‰¹å¤§å°')
    parser.add_argument('--lr', type=float, default=5e-5, help='å­¦ä¹ ç‡')
    parser.add_argument('--use_se_in_df', action='store_true', help='åœ¨DFä¸­ä½¿ç”¨SE Block')
    
    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    parser.add_argument('--use_distributed', action='store_true', help='å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒ')
    parser.add_argument('--gpus', nargs='+', type=int, default=[0], help='GPUåˆ—è¡¨')
    
    args = parser.parse_args()
    
    config = get_default_config()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.num_epochs:
        config['num_epochs'] = args.num_epochs
    if args.batch_size:
        config['batch_size'] = args.batch_size
    if args.lr:
        config['learning_rate'] = args.lr
    if args.use_se_in_df:
        config['use_se_in_df'] = True
    if args.use_distributed:
        config['use_distributed'] = True
    if args.gpus:
        config['gpus'] = args.gpus
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            user_config = json.load(f)
            config.update(user_config)
    
    # éªŒè¯GPUé…ç½®
    config, is_valid, error_msg = validate_gpu_config(config)
    if not is_valid:
        print(error_msg)
        return
    
    print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆè®­ç»ƒ...")
    print(f"ğŸ“‹ é…ç½®:")
    print(f"  - åˆ†å¸ƒå¼: {config['use_distributed']}")
    print(f"  - GPU: {config['gpus']}")
    print(f"  - DFä½¿ç”¨SE: {config['use_se_in_df']}")
    
    # å¯åŠ¨è®­ç»ƒ
    if config['use_distributed']:
        world_size = len(config['gpus'])
        try:
            mp.spawn(
                run_distributed_training,
                args=(world_size, config),
                nprocs=world_size,
                join=True
            )
            print("ğŸ‰ åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼")
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
    else:
        try:
            if torch.cuda.is_available() and config['gpus']:
                torch.cuda.set_device(config['gpus'][0])
            
            trainer = EnhancedTrainer(config)
            trainer.setup_data_loaders()
            trainer.setup_model()
            trainer.setup_loss_function()
            trainer.setup_optimizer()
            trainer.setup_logging()
            trainer.train()
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")


if __name__ == '__main__':
    main()


